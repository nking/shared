measurements taken using computer w/
    level 3 Cache: 3 MB shared (not used here)
    Cache Level 1:  32k for each core
    Cache Level 2:  256k for each core
    max memory bus: 26 GB/sec
    processor : 1.6 GHz

    at each cycle, memory can transfer at most:
        max memory bus (26 GB/sec) * (sec/1.6G cycles) = 16 Bytes / cycle
          = 128 bits / cycle
        and max memory bus rate is possibly not the mean rate.

using timing logs from the ant target cmakeSIMDTimeLogs:

---------------------------------------------------
for vec8prod:

each simd intrinsics 8-wide method invocation:
      1 load
      3 loadpermute 
      1 store
      3 mult (flops)
    ==>  avg measured load/store = 1.4 cycles
    ==>  avg measured 1 thread work = 3.3 cycles
         expect 1 thread work total = 5*1.4 + 3*mult
         estimate time for intrinsics flop =
              = (thread avg- 5*1.4)/(3. mult ops) = (3.3 - 5*1.4)/3.
    ==>       ~ 0

    arithmetic intensity = flops/Byte = 3/5, so is memory bound.

the serial mode for calculating the product of vector:
   = 324 cycles total
     so has rate 65536 float items/324 cycles = 325 floating point ops / cycle
the simd method for vector product:
   = 392043 cycles total
   N_threads used in that simd method  = 65536 float items/8-wide vectore 
      = 8192 (no thread pooling was used)

expect that the total 392043 cycles was composed of:  
      N_threads * (context switch + thread avg)
   estimate context switch = (392043 / 8192.) - 3.3
                     ~ 44 cycles
           which agrees w/ expected write to memory from cache being
           ~ 100 times a cache write ~ 100 ns on a COT computer
           (order of magnitude rough estimates)

we can expect that if we were to use simd serially, that
is if we have 65536 float elements and 8-wide simd vectors,
    we serially loop 8192 times 
      5 simd loads = 5*1.4 cycles
      3 simd mult ops ~ 0 cycles
    expect total time ~ 57344 cycles
     
    compare to serial w/o SIMD:  324 cycles
               multithread SIMD: 392043 cycles 

    ==> serial per element < serial SIMD < multithread SIMD in cycles needed

the serial SIMD algorithm on this computer is memory bandwidth bound.
(need to transfer >= 128 bits per cycle for SIMD)

* TODO: considering another multithread comparison that uses a threadpool
of size == number of computer cores.
   for c, can include thpool.c and thpool.h from
     https://github.com/Pithikos/C-Thread-Pool

* Consider how to improve an aspects of the SIMD version of vec8prod:

    Arithmetic (algorithmic) Intensity is the ratio of 
    total floating-point operations to total data movement (bytes)

To improve the arithmetic intensity of the SIMD vec8prod algorithm, we consider 
increasing the number of math ops per method or decreasing the denominator.
      3 ops / 5 loads.  

If we assume that reading sequentially into the SIMD vector for a larger
vector is roughly the same amount of time for load operations,
we don't change the Arithmetic Intensity, but we see that we can 
reduce the execution runtime, 
** but only if the computer's memory throughput can handle the 
data rate (otherwise, we are still memory bound and the Arith intensity 
estimate has to include the larger data load times and hence arith inten is reduced). **

If we increase the SIMD vector width without comparable increase in the
data load/store operation time (cycles), The number of operations of 
the same AlgInt (=3ops/5loads) is reduced, so the total execution runtime 
is reduced.

Because the work is independent, no overlap of data between workers, 
we don't consider improvements that reuse cached contents 
(e.g. fusing loop iterations or tiling)
except that a SIMD vector width must fit within the cache.

* If the amount of data to process can fit into memory of 1 computer,
this SIMD implementation of vec8prod is not as fast or efficient as
an element-wise serial approach when the computer is memory bound.

* This implementation of vec8prod does independent work and so is
  highly parallelizable.

If the total data is too large to load into an array to give to 
vec8prod, we consider parallel options.

But, the per element serial option on each partition is still faster
than the SIMD implementations for my implementation of this algorithm
with memory bound bottleneck. 

Continuing the line of thought in using the SIMD vec8prod in parallel:

  A combiner is needed to multiply the results of the independent
  workers.

  ** If the architecture is using shared memory, each worker can share a
  results array and place their results into it at their thread index defined
  index.

  ** If the architecture is not using shared memory, the results need to
  be communicated in some manner to a combiner.
  
  The combiner can use a serial product algorithm if all fits in memory
  or can parallelize the results vector and collect the result, etc.
  
* NOTE: the ISPC version of vec8prod can outperform the per element serial
  calculation.  see the ISPC directory.

-------------------------------------------------------------
wrote a quick algorithm to look at the improvements
in execution runtime using simd intrinsic
by increasing the arithmetic intensity.

for cache_explore_serial_more_math()
   and cache_explore_simd_more_math()

there are 
   20 math ops
   the input is a float array of 250000 elements.

  Arith intensity of serial per element:
      (20 flops / 4 Bytes)  over 250,000 loops

  Arith intensity of serial intrinsics (4-wide 32 bit vectors):
     (20 flops / 16 Bytes)  over 6250 loops

serial per element completes its work in ~ 20,000 cycles
serial simd 4-wide vector completes its work in ~ 4200 cycles
   
serial simd is a factor of 5 improvement over serial per element.

The better throughput (faster finish time) of the serial SIMD
suggests that serial per element alg is
** computation bound ** and higher than the computer's balance point.

 The computer's balance point can be seen in the roofline plot
 for the computer (which I don't have).