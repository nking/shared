{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hnp42Az-NgTq",
   "metadata": {
    "id": "hnp42Az-NgTq"
   },
   "source": [
    "This notebook is adapted from a copy of diffusers_training_example.ipynb\n",
    "The original file is located at\n",
    "    [this link](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb)\n",
    "\n",
    "These changes have been made to the tutuorial:\n",
    "\n",
    "* have edited the notebook to run a very small subset all the way through to make it easier to make changes for updated API\n",
    "arguments and data directories.\n",
    "* have added some math details to tie into a background of noise filtering algorithms.  These are at the bottom of this notebook.\n",
    "* have added a method for noise-filtering, that is, image restoration,\n",
    "to explore whether the algorithm can remove noise from a real image (like BM3D, etc.).\n",
    "It's a different use case than generating or predicting an image from pure noise.\n",
    "* have commented out the ability to share saved checkpoints, etc with Hugging Face while editing, but you can re-enable these.\n",
    "\n",
    "For reference, the DDPM paper is\n",
    "Ho, J., Jain, A. and Abbeel, P., 2020. Denoising diffusion probabilistic models.\n",
    "Advances in Neural Information Processing Systems, 33, pp.6840-6851."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jWWploJLQime",
   "metadata": {
    "id": "jWWploJLQime"
   },
   "source": [
    "------\n",
    "To run the code on the full dataset **after** you've made changes for data directories and API changes,\n",
    "**set run_small_inspect = False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1c59c-4499-4c1b-b554-5b9755035b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sagemaker or your desktop, may need to uncomment this (or add to a lifecycle script)\n",
    "#%pip install torch===2.1.1 torchvision===0.16.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bYPgC9tXQzSR",
   "metadata": {
    "id": "bYPgC9tXQzSR"
   },
   "outputs": [],
   "source": [
    "# to run a small subset of the data, and a small number of epochs and iterations\n",
    "# set run_small_inspect to True:\n",
    "run_small_inspect = True\n",
    "\n",
    "use_accelerator = False\n",
    "\n",
    "if run_small_inspect:\n",
    "    print(f'WARNING: run_small_inspect=True, (uses small subset of data, small # train epochs small # sample iterations')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cac24c-df25-49a7-9828-5518a31df8bc",
   "metadata": {
    "id": "34cac24c-df25-49a7-9828-5518a31df8bc"
   },
   "source": [
    "# ðŸ¤— Training with Diffusers\n",
    "\n",
    "In recent months, it has become clear that diffusion models have taken the throne as the state-of-the-art generative models. Here, we will use Hugging Face's brand new [Diffusers](https://github.com/huggingface/diffusers) library to train a simple diffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea0189-8329-49be-830a-3e22f50ee3c5",
   "metadata": {
    "id": "69ea0189-8329-49be-830a-3e22f50ee3c5"
   },
   "source": [
    "## Installing the dependencies\n",
    "\n",
    "This notebook leverages the [ðŸ¤— Datasets](https://huggingface.co/docs/datasets/index) library to load and preprocess image datasets and the [ðŸ¤— Accelerate](https://huggingface.co/docs/accelerate/index) library to simplify training on any number of GPUs, with features like automatic gradient accumulation and tensorboard logging. Let's install them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dnwEwhV4SrD",
   "metadata": {
    "id": "4dnwEwhV4SrD"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install git+https://github.com/huggingface/diffusers.git#egg=diffusers[training]\n",
    "%pip install accelerate\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tIYT2Tn2Q4cr",
   "metadata": {
    "id": "tIYT2Tn2Q4cr"
   },
   "source": [
    "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
    "\n",
    "To be able to share your model with the community, there are a few more steps to follow.|\n",
    "\n",
    "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your **write** token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-vQPIkaSRDKF",
   "metadata": {
    "id": "-vQPIkaSRDKF"
   },
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fgMTbF9XRH_b",
   "metadata": {
    "id": "fgMTbF9XRH_b"
   },
   "source": [
    "\n",
    "Then you need to install Git-LFS to upload your model checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b562u6iRF9G",
   "metadata": {
    "id": "4b562u6iRF9G"
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#%sudo apt -qq install git-lfs\n",
    "#%git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6312e0aa-a48b-4990-ab16-4f3952ee2eb7",
   "metadata": {
    "id": "6312e0aa-a48b-4990-ab16-4f3952ee2eb7"
   },
   "source": [
    "## Config\n",
    "\n",
    "For convenience, we define a configuration grouping all the training hyperparameters. This would be similar to the arguments used for a [training script](https://github.com/huggingface/diffusers/tree/main/examples).\n",
    "Here we choose reasonable defaults for hyperparameters like `num_epochs`, `learning_rate`, `lr_warmup_steps`, but feel free to adjust them if you train on your own dataset. For example, `num_epochs` can be increased to 100 for better visual quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4e50c-23d9-4057-ae37-c954d7e063bb",
   "metadata": {
    "id": "09f4e50c-23d9-4057-ae37-c954d7e063bb"
   },
   "source": [
    "## Loading the dataset\n",
    "\n",
    "We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download our image dataset.\n",
    "\n",
    "In this case, the [Butterflies dataset](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset) is hosted remotely, but you can load a local [ImageFolder](https://huggingface.co/docs/datasets/v2.0.0/en/image_process#imagefolder) as shown in the commets below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab38ad6-8ead-48d5-a5c8-a9b602e94bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f740dfe-e610-4479-ac30-cce1f9e62553",
   "metadata": {
    "id": "1f740dfe-e610-4479-ac30-cce1f9e62553"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Feel free to try other datasets from https://hf.co/huggan/ too!\n",
    "hf_name = \"huggan/smithsonian_butterflies_subset\"\n",
    "\n",
    "#if using in sagemaker jupyterlab, comment out this try/except block and add\n",
    "# data_dir = 'data'; out_data_dir = 'output' or your prefered names\n",
    "try:\n",
    "    data_dir = os.environ[\"ML_DATASETS_HOME\"]\n",
    "    out_data_dir = os.environ[\"ML_OUTPUT_HOME\"]\n",
    "except:\n",
    "    import tempfile\n",
    "    from tempfile import TemporaryDirectory\n",
    "    # colab environment\n",
    "    # other args: url, folder_in_archive\n",
    "    data_dir = tempfile.mkdtemp()\n",
    "    print(f'temp_dataset_dir={data_dir}')\n",
    "    out_data_dir = os.path.join(data_dir, \"output\")\n",
    "\n",
    "data_dir = os.path.join(data_dir, hf_name)\n",
    "out_data_dir = os.path.join(out_data_dir, hf_name)\n",
    "if run_small_inspect:\n",
    "    out_data_dir = out_data_dir + \"_small\"\n",
    "\n",
    "print(f'data_dir is {data_dir}')\n",
    "print(f'out_data_dir is {out_data_dir}')\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # even for ml.m5.xlarge, batch_size should be 4 unless you re-configure to use accelerate API\n",
    "    image_size = 128  # the generated image resolution\n",
    "    # have reduced the batch sizes to reduce use of memory (RAM + disk)\n",
    "    train_batch_size = 4#16\n",
    "    eval_batch_size = 4#16  # how many images to sample during evaluation\n",
    "    num_epochs = 50\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    dataset = hf_name\n",
    "    output_dir = out_data_dir  # the model name locally and on the HF Hub\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    hub_private_repo = False\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "    num_train_timesteps = 1000\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "if run_small_inspect:\n",
    "    config.num_epochs = 1\n",
    "    config.train_batch_size = 4\n",
    "    config.train_batch_size = 4\n",
    "    config.eval_batch_size = 4\n",
    "    config.num_train_timesteps = 1\n",
    "\n",
    "\"\"\"## Loading the dataset\n",
    "\n",
    "We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download our image dataset.\n",
    "\n",
    "In this case, the [Butterflies dataset](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset) is hosted remotely, but you can load a local [ImageFolder](https://huggingface.co/docs/datasets/v2.0.0/en/image_process#imagefolder) as shown in the commets below.\n",
    "\"\"\"\n",
    "from datasets import load_dataset\n",
    "\n",
    "if (not os.path.exists(data_dir)):\n",
    "    # butterflies\n",
    "    dataset = load_dataset(path=config.dataset, split=\"train\", keep_in_memory=False)\n",
    "    dataset.save_to_disk(data_dir)\n",
    "else:\n",
    "    from datasets import Dataset\n",
    "    #from torch.utils.data import Dataset\n",
    "    # keep_in_memory = False?\n",
    "    dataset = Dataset.load_from_disk(data_dir, keep_in_memory=False)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "if run_small_inspect:\n",
    "    # load only 1 batch to check pipeline\n",
    "    ind = np.array([i for i in range(0, config.train_batch_size)], dtype=np.int32)\n",
    "    indices = torch.from_numpy(ind)\n",
    "    indices2 = torch.from_numpy(np.array([i for i in range(config.train_batch_size-1, config.train_batch_size+2)], dtype=np.int32))\n",
    "    dataset_train = torch.utils.data.Subset(dataset, indices)\n",
    "    dataset_test = torch.utils.data.Subset(dataset, indices2)\n",
    "    dataset = dataset_train\n",
    "else :\n",
    "    ntr = int(0.8*len(dataset))\n",
    "    ind = np.array([i for i in range(0, ntr-2)], dtype=np.int32)\n",
    "    # include the last training image in test too to look at effect on known image\n",
    "    ind2 = np.array([i for i in range(ntr-3, len(dataset))], dtype=np.int32)\n",
    "    indices = torch.from_numpy(ind)\n",
    "    indices2 = torch.from_numpy(ind2)\n",
    "    dataset_train = torch.utils.data.Subset(dataset, indices)\n",
    "    dataset_test = torch.utils.data.Subset(dataset, indices2)\n",
    "    dataset = dataset_train\n",
    "\n",
    "'''\n",
    "The dataset contains several extra `features` (columns), but the one that we're interested in is `image`:\n",
    "Dataset({\n",
    "    features: ['image_url', 'image_alt', 'id', 'name', 'scientific_name', 'gender', 'taxonomy', 'region', 'locality',\n",
    "    'date', 'usnm_no', 'guid', 'edan_url', 'source', 'stage', 'image', 'image_hash', 'sim_score'],\n",
    "    num_rows: 1000\n",
    "})\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tRg5MsxBaHFk",
   "metadata": {
    "id": "tRg5MsxBaHFk"
   },
   "source": [
    "Since the [`Image`](https://huggingface.co/docs/datasets/image_process#image-datasets) feature loads the images with PIL, we can easily look at a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AchvRobca31r",
   "metadata": {
    "id": "AchvRobca31r"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nt = min(len(dataset), 4)\n",
    "fig, axs = plt.subplots(1, nt, figsize=(16, 4))\n",
    "for i, image in enumerate(dataset[:nt][\"image\"]):\n",
    "    axs[i].imshow(image)\n",
    "    axs[i].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BTMWxFwgj5xh",
   "metadata": {
    "id": "BTMWxFwgj5xh"
   },
   "source": [
    "The images in the dataset are all different, so we need to preprocess them first:\n",
    "* `Resize` makes the images conform to a square resolution of `config.image_size`\n",
    "* `RandomHorizontalFlip` augments the dataset by randomly mirroring the images.\n",
    "* `Normalize` is important to rescale the pixel values into a `[-1, 1]` range (which our model will expect)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ELMRWDKxfVXw",
   "metadata": {
    "id": "ELMRWDKxfVXw"
   },
   "source": [
    "ðŸ¤— Datasets offer a handy `set_transform()` method to apply the image transformations on the fly during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83158bea-273e-4088-a75a-e3c7cc86e0fc",
   "metadata": {
    "id": "83158bea-273e-4088-a75a-e3c7cc86e0fc"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((config.image_size, config.image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\"\"\"ðŸ¤— Datasets offer a handy `set_transform()` method to apply the image transformations on the fly during training:\"\"\"\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "def transform_test(examples):\n",
    "    preprocess_test = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((config.image_size, config.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "    images = [preprocess_test(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "try:\n",
    "    dataset.set_transform(transform)\n",
    "    dataset_test.set_transform(transform_test)\n",
    "except:\n",
    "    #for Subset\n",
    "    dataset.dataset.set_transform(transform)\n",
    "    dataset_test.dataset.set_transform(transform_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GqCHZsjnfcW1",
   "metadata": {
    "id": "GqCHZsjnfcW1"
   },
   "source": [
    "Let's see what they look like now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3YcuQZXafk0g",
   "metadata": {
    "id": "3YcuQZXafk0g"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, nt, figsize=(16, 4))\n",
    "for i, image in enumerate(dataset[:nt][\"images\"]):\n",
    "    axs[i].imshow(image.permute(1, 2, 0).numpy() / 2 + 0.5)\n",
    "    axs[i].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oiR0bNZ-plUL",
   "metadata": {
    "id": "oiR0bNZ-plUL"
   },
   "source": [
    "Now that all our images have the same size and are converted to tensors, we can create the dataloader we will use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rjBFh_8HpVam",
   "metadata": {
    "id": "rjBFh_8HpVam"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584739c5-83f5-4077-8379-89a7c9ff1bf2",
   "metadata": {
    "id": "584739c5-83f5-4077-8379-89a7c9ff1bf2"
   },
   "source": [
    "## Defining the diffusion model\n",
    "\n",
    "Here we set up our diffusion model. Diffusion models are neural networks that are trained to predict slightly less noisy images from a noisy input. At inference, they can be used to iteratively transform a random noise to generate an image:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/10695622/174349667-04e9e485-793b-429a-affe-096e8199ad5b.png\" width=\"800\"/>\n",
    "    <br>\n",
    "    <em> Figure from DDPM paper (https://arxiv.org/abs/2006.11239). </em>\n",
    "<p>\n",
    "\n",
    "Don't worry too much about the math if you're not familiar with it, the import part to remember is that our model corresponds to the arrow $p_{\\theta}(x_{t-1}|x_{t})$ (which is a fancy way of saying: predict a slightly less noisy image).\n",
    "\n",
    "The interesting part is that it's really easy to add some noise to an image, so the training can happen in a semi-supervised fashion as follows:\n",
    "1. Take an image from the training set.\n",
    "2. Apply to it some random noise $t$ times (this will give the $x_{t-1}$ and the $x_{t}$ in the figure above).\n",
    "3. Give this noisy image to the model along with the value of $t$.\n",
    "4. Compute a loss from the output of the model and the noised image $x_{t-1}$.\n",
    "\n",
    "Then we can apply gradient descent and repeat this process multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5Xl7Zd72qp8w",
   "metadata": {
    "id": "5Xl7Zd72qp8w"
   },
   "source": [
    "Most diffusion models use architectures that are some variant of a [U-net](https://arxiv.org/abs/1505.04597) and that's what we'll use here.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/unet-model.png)\n",
    "\n",
    "In a nutshell:\n",
    "- the model has the input image go through several blocks of ResNet layers which halves the image size by 2\n",
    "- then through the same number of blocks that upsample it again.\n",
    "- there are skip connections linking the features on the downample path to the corresponsding layers in the upsample path.\n",
    "\n",
    "A key feature of this model is that it predicts images of the same size as the input, which is exactly what we need here.\n",
    "\n",
    "Diffusers provides us a handy `UNet2DModel` class which creates the desired architecture in PyTorch.\n",
    "\n",
    "Let's create a U-net for our desired image size.\n",
    "Note that `down_block_types` correspond to the downsampling blocks (green on the diagram above), and `up_block_types` are the upsampling blocks (red on the diagram):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eb5811-c10b-4dae-a58d-9583c42e7f57",
   "metadata": {
    "id": "e3eb5811-c10b-4dae-a58d-9583c42e7f57"
   },
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "# documentation https://huggingface.co/docs/diffusers/api/models/unet2d\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=3,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channes for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\"\n",
    "      ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XVFZ9G6r6Zl6",
   "metadata": {
    "id": "XVFZ9G6r6Zl6"
   },
   "source": [
    "Let's get a sample image from our dataset and pass it into our model. We just need to add a batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd51b19-c237-4ddf-be90-b22db18f919f",
   "metadata": {
    "id": "7cd51b19-c237-4ddf-be90-b22db18f919f"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sample_image = dataset[0]['images'].unsqueeze(0)\n",
    "except:\n",
    "    #Subset\n",
    "    sample_image = dataset.dataset[0]['images'].unsqueeze(0)\n",
    "print('Input shape:', sample_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ZwaxdNntjj4",
   "metadata": {
    "id": "7ZwaxdNntjj4"
   },
   "source": [
    "And let's check the output is a tensor of the same exact shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34886ac-fe74-4624-8faa-8236616babc4",
   "metadata": {
    "id": "c34886ac-fe74-4624-8faa-8236616babc4"
   },
   "outputs": [],
   "source": [
    "print('Output shape:', model(sample_image, timestep=0)[\"sample\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e22892-18b6-44b1-a197-46d23a70e970",
   "metadata": {
    "id": "15e22892-18b6-44b1-a197-46d23a70e970"
   },
   "source": [
    "Great!\n",
    "\n",
    "Note that our model takes in the (noisy) image and also the current time-step (as we saw before in the training overview). That time-step information is converted for the model using a sinusoidal positional embedding, similar to what Transformer models often do.\n",
    "\n",
    "Now that we have our model, we just need an object to *add noise to an image*. This is done by the **schedulers** in the `diffusers` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ea3716-a22a-45d4-a8e9-3f513c77c35c",
   "metadata": {
    "id": "85ea3716-a22a-45d4-a8e9-3f513c77c35c"
   },
   "source": [
    "## Defining the noise scheduler\n",
    "\n",
    "Depending on the diffusion algorithm you want to use, the way images are noised is slightly different. That's why ðŸ¤— Diffusers contains different scheduler classes which each define the algorithm-specific diffusion steps. Here we are going to use the `DDPMScheduler` which corresponds to the training denoising and training algorithm proposed in [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a059d-a849-449e-8e11-72abc9c9fe2d",
   "metadata": {
    "id": "575a059d-a849-449e-8e11-72abc9c9fe2d"
   },
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "\n",
    "#noise_scheduler = DDPMScheduler(num_train_timesteps=1000, tensor_format=\"pt\")\n",
    "noise_scheduler = DDPMScheduler()\n",
    "noise_scheduler.config.num_train_timesteps = config.num_train_timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05507ad-0898-4e36-891a-03d88e8cfb10",
   "metadata": {
    "id": "a05507ad-0898-4e36-891a-03d88e8cfb10"
   },
   "source": [
    "Let's see how this noise scheduler works: it takes a batch of images from the trainng set (here we will reuse the batch of one image `sample_image` form before), a batch of random noise of the same shape and the timesteps for each image (which correspond to the number of times we want to apply noise to each image):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SWIDblOdu4Sg",
   "metadata": {
    "id": "SWIDblOdu4Sg"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "noise = torch.randn(sample_image.shape)\n",
    "timesteps = torch.LongTensor([50])\n",
    "noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
    "\n",
    "Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QNLSQtrpvtcu",
   "metadata": {
    "id": "QNLSQtrpvtcu"
   },
   "source": [
    "In the DDPM algorithm, the training objective of the model is then to be able to predict the noise we used in `noise_scheduler.add_noise`, so the loss at this step would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JsK5WWwvv8gM",
   "metadata": {
    "id": "JsK5WWwvv8gM"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "noise_pred = model(noisy_image, timesteps)[\"sample\"]\n",
    "loss = F.mse_loss(noise_pred, noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c293be5-367f-4606-9d9c-ac3783817ba9",
   "metadata": {
    "id": "6c293be5-367f-4606-9d9c-ac3783817ba9"
   },
   "source": [
    "## Setting up training\n",
    "\n",
    "We have all we need to be able to train our model! Let's use a standard AdamW optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb43de-6e4f-43f3-929c-aae82b1c648b",
   "metadata": {
    "id": "ddbb43de-6e4f-43f3-929c-aae82b1c648b"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AjyzINS3wWba",
   "metadata": {
    "id": "AjyzINS3wWba"
   },
   "source": [
    " And a cosine learning rate schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ae2a8-9e72-4be2-8954-b8532cf323ee",
   "metadata": {
    "id": "a60ae2a8-9e72-4be2-8954-b8532cf323ee"
   },
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "if run_small_inspect:\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=config.lr_warmup_steps,\n",
    "        num_training_steps=(config.num_epochs),\n",
    "    )\n",
    "else:\n",
    "    if use_accelerator:\n",
    "        lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=config.lr_warmup_steps,\n",
    "            num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    "        )\n",
    "    else:\n",
    "        lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=config.lr_warmup_steps,\n",
    "            num_training_steps=(config.num_epochs),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f9523b-2547-4706-955c-7621ea70a389",
   "metadata": {
    "id": "67f9523b-2547-4706-955c-7621ea70a389"
   },
   "source": [
    "To evaluate our model, we use the `DDPMPipeline` which is an easy way to perform end-to-end inference (see this notebook [TODO link] for more detail). We will use this pipeline to generate a batch of sample images and save it as a grid to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pwxUwlLBw-O1",
   "metadata": {
    "id": "pwxUwlLBw-O1"
   },
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "\n",
    "import math\n",
    "\n",
    "def make_grid(images, rows, cols):\n",
    "    w, h = images[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    print(f'evaluate {epoch}')\n",
    "    # default num_inference_steps=1000\n",
    "\n",
    "    # the pipeline runs with torch.no_grad(), so is not training, just predicting\n",
    "    result = pipeline(\n",
    "        batch_size = config.eval_batch_size,\n",
    "        generator=torch.manual_seed(config.seed),\n",
    "        num_inference_steps=noise_scheduler.config.num_train_timesteps,\n",
    "    )\n",
    "\n",
    "    # result type is ImagePipelineOutput\n",
    "    images = result[\"images\"]\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    image_grid = make_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    print('save sampled images to disk')\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "my90vVcmxU5V",
   "metadata": {
    "id": "my90vVcmxU5V"
   },
   "source": [
    "With this in end, we can group all together and write our training function. This just wraps the training step we saw in the previous section in a loop, using Accelerate for easy TensorBoard logging, gradient accumulation, mixed precision training and multi-GPUs or TPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67640279-979b-490d-80fe-65673b94ae00",
   "metadata": {
    "id": "67640279-979b-490d-80fe-65673b94ae00"
   },
   "outputs": [],
   "source": [
    "import diffusers\n",
    "#from diffusers.utils.hub_utils import init_git_repo, push_to_hub\n",
    "\n",
    "if use_accelerator:\n",
    "    from accelerate import Accelerator\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "def train_loop_with_accelerator(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "\n",
    "    # TODO: may need to update this method\n",
    "\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        #logging_dir=os.path.join(config.output_dir, \"logs\")\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\")\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        #if config.push_to_hub:\n",
    "        #    repo = init_git_repo(config, at_init=True)\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, you just need to unpack the\n",
    "    # objects in the same order you gave them to the prepare method.\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch['images']\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(noisy_images, timesteps)[\"sample\"]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "\n",
    "            # postpone evaluate until after training\n",
    "            #if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            #    evaluate(config, epoch, pipeline)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                #if config.push_to_hub:\n",
    "                #    push_to_hub(config, pipeline, repo, commit_message=f\"Epoch {epoch}\", blocking=True)\n",
    "                #else:\n",
    "                #    pipeline.save_pretrained(config.output_dir)\n",
    "                pipeline.save_pretrained(config.output_dir)\n",
    "\n",
    "def train_loop_no_accelerator(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    import datetime\n",
    "    print('start train loop')\n",
    "    #config.mixed_precision,\n",
    "    #config.gradient_accumulation_steps,\n",
    "\n",
    "    size = len(train_dataloader.dataset)\n",
    "    model.train()  # sets the module to training mode\n",
    "\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch['images'] #(16,3,128,128)\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the \"forward\" diffusion process)\n",
    "            # alphas are kept in noise_scheduler.\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            # the model learns the \"reverse\" process, given a corrupted image generate\n",
    "            # a less corrupted image or generate the noise between them in this case.\n",
    "\n",
    "            # Predict the noise residual\n",
    "            noise_pred = model(noisy_images, timesteps)[\"sample\"]\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "            # TODO: consider plotting (clean_images - noise_pred) as training progresses.\n",
    "\n",
    "            # Backpropagation\n",
    "            if run_small_inspect:\n",
    "                print('backpropagation', flush=True)\n",
    "            loss.backward()\n",
    "            if run_small_inspect:\n",
    "                print('update params', flush=True)\n",
    "            # update the parameters (weights and biases)  of UNet2DModel indirectly through optimizer\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if run_small_inspect or step % 100 == 0:\n",
    "                current_time = datetime.datetime.now()\n",
    "                formatted_time = current_time.strftime('%H:%M:%S')\n",
    "                print(formatted_time)\n",
    "                loss = loss.detach().item()\n",
    "                print(f\"loss: {loss:>7f}  [{step:>5d}/{size:>5d}, {epoch}]\")\n",
    "\n",
    "        # after an epoch of train and eval, let the regularization adapt:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # if this is the last epoch, run the computationally expensive evaluation (sampling is expensive).\n",
    "        if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            print('construct pipeline')\n",
    "            # pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "            pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n",
    "\n",
    "            # if config.push_to_hub:\n",
    "            #    push_to_hub(config, pipeline, repo, commit_message=f\"Epoch {epoch}\", blocking=True)\n",
    "            # else:\n",
    "            #    pipeline.save_pretrained(config.output_dir)\n",
    "            print('save pipeline configuration')\n",
    "            pipeline.save_pretrained(config.output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fbb01c-1d64-4496-a9f1-5799051c1032",
   "metadata": {
    "id": "69fbb01c-1d64-4496-a9f1-5799051c1032"
   },
   "source": [
    "## Let's train!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ba8b7-eb8f-4e8e-88ae-6e4a2b68433e",
   "metadata": {
    "id": "b11ba8b7-eb8f-4e8e-88ae-6e4a2b68433e"
   },
   "outputs": [],
   "source": [
    "# to reload from pretrained model, use False here:\n",
    "if True:\n",
    "    if use_accelerator:\n",
    "        from accelerate import notebook_launcher\n",
    "        args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "        ## launch the training (including multi-GPU training) from the notebook using Accelerate's `notebook_launcher` function:\n",
    "        notebook_launcher(train_loop_with_accelerator, args, num_processes=1)\n",
    "        ## to use without notebook_launcher:\n",
    "        #train_loop_with_accelerator(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "    else:\n",
    "        train_loop_no_accelerator(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "    print('done training')\n",
    "else:\n",
    "    #load pretrained pipeline\n",
    "    print(f'load pretrained pipeline from {config.output_dir}')\n",
    "    try:\n",
    "        pipeline = DDPMPipeline.from_pretrained(config.output_dir)\n",
    "        model = pipeline.unet\n",
    "        noise_scheduler = pipeline.scheduler\n",
    "        if model is None or noise_scheduler is None:\n",
    "           raise Exception(\"pipeline load doesn't include model and scheduler load, so loading those next\")\n",
    "    except:\n",
    "        print(f'load pretrained model and scheduler')\n",
    "        model = UNet2DModel.from_pretrained(config.output_dir, subfolder='unet')\n",
    "        noise_scheduler = DDPMScheduler.from_pretrained(config.output_dir, subfolder='scheduler')\n",
    "        pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W6pK8RonfSu0",
   "metadata": {
    "id": "W6pK8RonfSu0"
   },
   "source": [
    "These are 2 separate use-cases for the trained model.\n",
    "\n",
    "* generate_sample() inputs noise and the model predicts what the restored image is.  It manufactures an image based on the world of the training dataset.\n",
    "\n",
    "* filter_noise() inputs a test image and the model predicts what the uncorrupted image should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r5PM6vOQPISl",
   "metadata": {
    "id": "r5PM6vOQPISl"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "for tmp in [f\"{config.output_dir}/images/\", f\"{config.output_dir}/de-noised/\", f\"{config.output_dir}/noisy-images/\"]:\n",
    "    %rm -rf {tmp}\n",
    "\n",
    "def add_noise_to_images(image_batch, n_noise: int) :\n",
    "    # checking: these are in range [-1,1]\n",
    "    #min_value = torch.min(image_batch)\n",
    "    #max_value = torch.max(image_batch)\n",
    "    s0 = image_batch.shape[0] #batch_size\n",
    "    s2 = image_batch.shape[2] #rows\n",
    "    s3 = image_batch.shape[3] #cols\n",
    "    #TODO: rewrite this in more efficient way\n",
    "    # image_batch shape is (4,3,128,128)\n",
    "    for i in range(n_noise):\n",
    "        # Get a random index for each image in the batch\n",
    "        random_row_indices = torch.randint(0, s2, (s0,))\n",
    "        random_col_indices = torch.randint(0, s3, (s0,))\n",
    "        # generate random r,g,b values for those pixels. noise will be 10% of these\n",
    "        random_r = 0.1*torch.randn(s0)\n",
    "        random_g = 0.1*torch.randn(s0)\n",
    "        random_b = 0.1*torch.randn(s0)\n",
    "        for j in range(s0):\n",
    "            image_batch[j][0][random_row_indices[j]][random_col_indices[j]] += random_r[j]\n",
    "            image_batch[j][1][random_row_indices[j]][random_col_indices[j]] += random_g[j]\n",
    "            image_batch[j][2][random_row_indices[j]][random_col_indices[j]] += random_b[j]\n",
    "    image_batch.clamp_(-1, 1)\n",
    "\n",
    "def generate_sample() :\n",
    "    '''\n",
    "    apply the trained U-Net model, UNet2DModel, and alpha coefficients to images of\n",
    "    pure noise.  the result is the creation of signal in the image. the final image\n",
    "    belongs to same probability distribution as the original images.\n",
    "    '''\n",
    "    print('evaluate generative pipeline')\n",
    "    pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n",
    "\n",
    "    samples_dir = os.path.join(config.output_dir, \"samples\")\n",
    "\n",
    "    #if (not os.path.exists(samples_dir)) or (len(os.listdir(samples_dir))== 0):\n",
    "    evaluate(config, config.num_epochs, pipeline)\n",
    "\n",
    "    sample_images = sorted(glob.glob(f\"{samples_dir}/*.png\"))\n",
    "\n",
    "    Image.open(sample_images[-1])\n",
    "    print('done with display sample generated images')\n",
    "\n",
    "def save_images(batch : torch.Tensor, file_prefix: str):\n",
    "    imgs = diffusers.utils.pt_to_pil(batch)\n",
    "    for i, img in enumerate(imgs):\n",
    "        file_name = f\"{file_prefix}_{i:02d}.png\"\n",
    "        #print(f'writing to {file_name}')\n",
    "        img.save(f\"{file_name}\")\n",
    "\n",
    "def filter_noise(add_noise:bool=False, noise_type:str=None):\n",
    "    '''\n",
    "    apply the trained U-Net model, UNet2DModel, and alpha coefficients to the original images\n",
    "    to restore an image corrupted by noise.  The input images used are the training images.\n",
    "    one could substitute other images here.\n",
    "\n",
    "    Args:\n",
    "       add_noise: bool\n",
    "          if False, the test images are used as is,\n",
    "          else if True, noise is added to the test images\n",
    "          and the type of noise is specified by argument noise_type\n",
    "       noise_type: str\n",
    "          if 'ddpm' then noise generated by DDPMScheduler is added to test images,\n",
    "          else default = 'rand' and random noise is added to random pixels of the\n",
    "          test images at 10% level\n",
    "\n",
    "    TODO: to predict and image instead of noise, could subclass DDPMPipeline\n",
    "          to add a method for it.   In that new method\n",
    "          the sigma_t term possibly needs to be set to beta_t * (1- (alpha_bar_{t-1}))/(1-(alpha_bar_{t}))\n",
    "          see Ho et al. 2020 eqn 8.\n",
    "\n",
    "    NOTE: test dataset has 1 image from train dataset and 2 that are not from training dataset\n",
    "    '''\n",
    "    import math\n",
    "\n",
    "    if add_noise and noise_type is not None and (noise_type != 'ddpm' and noise_type != 'rand'):\n",
    "        raise ValueError(\"when add_noise and noise_type are set, noise_type must be either 'ddpm' or 'rand'\")\n",
    "\n",
    "    dirname = \"de-noised\"\n",
    "    img_dir = os.path.join(config.output_dir, dirname)\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "    img0_dir = os.path.join(config.output_dir, \"images\")\n",
    "    os.makedirs(img0_dir, exist_ok=True)\n",
    "    print(f'apply de-noise filtering to test images')\n",
    "\n",
    "    timesteps = torch.from_numpy(np.arange(0, noise_scheduler.config.num_train_timesteps)[::-1].copy())\n",
    "\n",
    "    if add_noise:\n",
    "       os.makedirs(f\"{config.output_dir}/noisy-images/\", exist_ok=True)\n",
    "\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        image_batch = batch['images']  # (16,3,128,128)\n",
    "\n",
    "        save_images(batch=image_batch, file_prefix=f\"{config.output_dir}/images/{step:04d}\")\n",
    "\n",
    "        if add_noise:\n",
    "            image_batch = image_batch.clone()\n",
    "            if noise_type is not None and noise_type == 'ddpm':\n",
    "                noise = torch.randn(image_batch.shape).to(image_batch.device)\n",
    "                ts = torch.randint(0, noise_scheduler.config.num_train_timesteps,\n",
    "                    (image_batch.shape[0],), device=image_batch.device).long()\n",
    "                image_batch = noise_scheduler.add_noise(image_batch, noise, ts)\n",
    "            else:\n",
    "                add_noise_to_images(image_batch, int(100 * noise_scheduler.config.num_train_timesteps))\n",
    "            # save noisy image to file to compare afterwards\n",
    "            save_images(batch=image_batch, file_prefix=f\"{config.output_dir}/noisy-images/{step:04d}\")\n",
    "\n",
    "        ## diffusers.pipelines.pipeline_utils.DiffusionPipeline.progress_bar\n",
    "        for t in tqdm(timesteps):\n",
    "            with torch.no_grad():\n",
    "                # 1. predict noise model_output\n",
    "                model_output = model(image_batch, t).sample\n",
    "\n",
    "            # 2. compute previous image: x_t -> x_t-1 by reversing the stochastic differential equation.\n",
    "            # This function propagates the diffusion process from the learned model outputs\n",
    "            # (most often the predicted noise).\n",
    "            image_batch = noise_scheduler.step(model_output, t, image_batch,\n",
    "                    generator=torch.manual_seed(config.seed),).prev_sample\n",
    "\n",
    "            if t % 100 == 0:\n",
    "                save_images(batch=image_batch, file_prefix=f\"{img_dir}/{step:04d}_t{t:04d}\")\n",
    "\n",
    "        save_images(batch=image_batch, file_prefix=f\"{img_dir}/{step:04d}\")\n",
    "\n",
    "    filtered_images = sorted(glob.glob(f\"{img_dir}/*.png\"))\n",
    "    print(f'filtered_images')\n",
    "    #Image.open(filtered_images[-1])\n",
    "    for idx in range(0, len(filtered_images), 1):\n",
    "        im = Image.open(filtered_images[idx])\n",
    "        display(im)\n",
    "\n",
    "    print('done with display noise-filtered images')\n",
    "\n",
    "\n",
    "# these are 2 separate use-cases for the trained model\n",
    "filter_noise(add_noise = True)\n",
    "generate_sample()\n",
    "\n",
    "if run_small_inspect:\n",
    "    print(f'WARNING REMINDER: run_small_inspect=True, (uses small subset of data, small # train epochs small # sample iterations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N3HQsg3S0kQ_",
   "metadata": {
    "id": "N3HQsg3S0kQ_"
   },
   "outputs": [],
   "source": [
    "images = sorted(glob.glob(f\"{config.output_dir}/images/*.png\"))\n",
    "for image in images:\n",
    "    print(f'{image}:')\n",
    "    im = Image.open(image)\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dvvaB3wgnW0k",
   "metadata": {
    "id": "dvvaB3wgnW0k"
   },
   "source": [
    "*Plot noisy-images before de-noised*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cXHnwVnWHD",
   "metadata": {
    "id": "e3cXHnwVnWHD"
   },
   "outputs": [],
   "source": [
    "images = sorted(glob.glob(f\"{config.output_dir}/noisy-images/*.png\"))\n",
    "for image in images:\n",
    "    print(f'{image}:')\n",
    "    im = Image.open(image)\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qv7K7viln17c",
   "metadata": {
    "id": "Qv7K7viln17c"
   },
   "source": [
    "**Plot de-noised noisy-images**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dpauJAunxuZ",
   "metadata": {
    "id": "6dpauJAunxuZ"
   },
   "outputs": [],
   "source": [
    "images = sorted(glob.glob(f\"{config.output_dir}/de-noised/*.png\"))\n",
    "for image in images:\n",
    "    print(f'{image}:')\n",
    "    im = Image.open(image)\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AnSP0sNjoEWf",
   "metadata": {
    "id": "AnSP0sNjoEWf"
   },
   "source": [
    "**Plot generated samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U-WtRv62oUPf",
   "metadata": {
    "id": "U-WtRv62oUPf"
   },
   "outputs": [],
   "source": [
    "images = sorted(glob.glob(f\"{config.output_dir}/samples/*.png\"))\n",
    "for image in images:\n",
    "    print(f'{image}:')\n",
    "    im = Image.open(image)\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eNHC0CaoqGfR",
   "metadata": {
    "id": "eNHC0CaoqGfR"
   },
   "source": [
    "A quick quantification of difference between clean and de-noised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h4NmTRcx5vJC",
   "metadata": {
    "id": "h4NmTRcx5vJC"
   },
   "outputs": [],
   "source": [
    "def calculateMedianOfAbsoluteDeviation(a) :\n",
    "    a = np.sort(a)\n",
    "    n = len(a)\n",
    "    median = a[int(n/2)]\n",
    "    d = np.sort(np.abs(a - median))\n",
    "    mad = d[int(n/2)]\n",
    "    return (mad, median)\n",
    "\n",
    "images = sorted(glob.glob(f\"{config.output_dir}/images/*.png\"))\n",
    "msens = []\n",
    "msedns = []\n",
    "for filename in images:\n",
    "    fn = os.path.split(filename)[-1]\n",
    "    #TODO: edit for OS agnostic path composition here and above:\n",
    "    im0 = Image.open(f\"{config.output_dir}/images/{fn}\")\n",
    "    imdn = Image.open(f\"{config.output_dir}/de-noised/{fn}\")\n",
    "    imn = Image.open(f\"{config.output_dir}/noisy-images/{fn}\")\n",
    "    d0 = np.asarray(im0, dtype=np.float32)\n",
    "    ddn = np.asarray(imdn, dtype=np.float32)\n",
    "    dn = np.asarray(imn, dtype=np.float32)\n",
    "    n = np.size(d0)\n",
    "    msen = np.sum((dn - d0)**2)/n #mse  noisy image - uncorrupted image\n",
    "    msedn = np.sum((ddn - d0)**2)/n #mse  de-noised image - uncorrupted image\n",
    "    # msedn should be << msen\n",
    "    print(f'a = mse  noisy image - uncorrupted image = {msen:.4f}')\n",
    "    print(f'b = mse  de-noised image - uncorrupted image = {msedn:.4f}')\n",
    "    print(f'b should b << a;  b/a = {(msedn/msen):.4f} should be small')\n",
    "    msens.append(msen)\n",
    "    msedns.append(msedn)\n",
    "\n",
    "n = len(msedns)\n",
    "if n == 0:\n",
    "     raise ValueError(f\"test images were not written to {config.output_dir}/images/\")\n",
    "\n",
    "mean_msedn = sum(msedns)/n\n",
    "mean_msen = sum(msens)/n\n",
    "\n",
    "#stDev = 1.4826*MAD\n",
    "mad_n, median_n = calculateMedianOfAbsoluteDeviation(msens)\n",
    "mad_dn, median_dn = calculateMedianOfAbsoluteDeviation(msedns)\n",
    "print(f'mean of MSEs (de-noised - uncorrupted)/(noisy - uncorrupted)={(mean_msedn/mean_msen):.4f} should be small')\n",
    "print(f'median (noisy - uncorrupted) = {median_n:.4f} +- {1.4826*mad_n:.4f}')\n",
    "print(f'median (de-noised - uncorrupted) = {median_dn:.4f} +- {1.4826*mad_dn:.4f}')\n",
    "print(f'median (de-noised - uncorrupted) / median (noisy - uncorrupted) = {(median_dn/median_n):.4f}')\n",
    "\n",
    "print(f'an image corrupted by random gaussian noise that is 10% of signal uses a' +\n",
    "      f' different noise pattern.  Does the model remove the noise well for it?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smJeP67bF0yj",
   "metadata": {
    "id": "smJeP67bF0yj"
   },
   "source": [
    "Not bad! There's room for improvement of course, so feel free to play with the hyperparameters, model definition and image augmentations ðŸ¤—\n",
    "\n",
    "If you've chosen to upload the model to the Hugging Face Hub, its repository should now look like so:\n",
    "https://huggingface.co/anton-l/ddpm-butterflies-128\n",
    "\n",
    "If you want to dive deeper into the code, we also have more advanced training scripts with features like Exponential Moving Average of model weights here:\n",
    "\n",
    "https://github.com/huggingface/diffusers/tree/main/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5UKvSDwAOx7G",
   "metadata": {
    "id": "5UKvSDwAOx7G"
   },
   "source": [
    "---------\n",
    "# Misc notes on noise-filtering\n",
    "The DDPM paper reference is\n",
    "Ho, J., Jain, A. and Abbeel, P., 2020. Denoising diffusion probabilistic models.\n",
    "Advances in Neural Information Processing Systems, 33, pp.6840-6851.\n",
    "\n",
    "\n",
    "This tutorial's choice of training images is a set of well curated images of butterflies.\n",
    "One can also find a pretrained model from a set of cat images \"google/ddpm-cat-256\".\n",
    "The butterfly training uses many images that are carefully prepared to represent samples from the same distribution.\n",
    "The DDPM paper authors (Ho, Jain, Abbeel 2020) however, use images that are busy, uncentered,\n",
    "and do not have background removed, but are all of same distribution category (e.g. castles, bedrooms, etc).\n",
    "This distribution is part of the encoding for the continuous latent space.\n",
    "\n",
    "B.T.W. The white background of the training images makes it easier to see effects of\n",
    "applying the reverse process to the images (for filter_noise()).\n",
    "\n",
    "**Some details on noise removal algorithms:**\n",
    "\n",
    "The form of \"grouping and collaborative filtering\" to learn noise and remove it can be seen in\n",
    "the BM3D algorithm.\n",
    "Other noise-removal algorithms use wavelet transforms, Wiener, median, shrinkage-thresholding, linear and non-linear\n",
    "smoothing, statistical methods...\n",
    "\n",
    "A simple form of deconvolution is (https://en.m.wikipedia.org/wiki/Deconvolution):\n",
    "    \n",
    "\n",
    "```\n",
    "f * g = h\n",
    "    where f is source signal to recover,\n",
    "    g is distortion function or filter,\n",
    "    h is the observed signal\n",
    "    * is the convolution symbol\n",
    "\n",
    "(f * g) + eps = h\n",
    "    where eps is added noise\n",
    "\n",
    "if eps is small (or removed), we can recover f using fourier transforms\n",
    "followed by inverse fourier transform\n",
    "    H = FFT(h); G = FFT(g)\n",
    "```\n",
    "\n",
    "When modelling the noise to remove it, a common technique is to use a Weiner filter\n",
    "which assumes white noise, that is, a random signal with equal intensities at different frequencies\n",
    "(== constant power spectral density).\n",
    "\n",
    "DDPM uses a random signal and a random sampling of frequencies from a\n",
    "bounded frequency domain in its estimation of noise to add to images.  \n",
    "This is part of the 'forward' process of DDPM\n",
    "and is a cumulated product with re-parameterized parameters.\n",
    "\n",
    "\n",
    "The forward process takes an image and continuously adds gaussian noise to it.\n",
    "(remember that any underlying noise distribution will resemble a gaussian distribution\n",
    "as n increases, where n is the number of draws).\n",
    "\n",
    "The model learns the residual noise between the latest image and the latest image before\n",
    "this round of adding noise.  The loss is calculated and the model parameters are updated\n",
    "using the gradients of the loss.\n",
    "\n",
    "The trained model can be used to \"reverse\" the process of adding noise to an image.\n",
    "They call the process \"de-noising\" because they optimize a loss objective that resembles\n",
    "de-noising score matching over multiple score scales indexed by t.\n",
    "\n",
    "A tangent to explore why the model doesn't work well as a noise-filter in image restoration:\n",
    "The noise model is composed of an aggregate of noise added to batches of images.\n",
    "When the reverse process sees what it recognizes as noise in the input image,\n",
    "it does not try to remove it, but instead, tries to restore the noise pixels to a state it\n",
    "believes is before the noise was added.  The representation of the previous state for those\n",
    "pixels may be very different than an individual image's pixels' nearest neighbors when the\n",
    "individual starting input image is not pure noise.\n",
    "When the individual starting input image is pure noise, the restoration of pixels\n",
    "follows what the model has seen before essentially and generates an interesting image\n",
    "from the implicit forward posterior distribution and UNet + noise scheduler model of the noise\n",
    "in a decoding phase. see Sect 4.4 of paper.\n",
    "   To explore the concept more, we could restrict the images to being binary, k = 2,\n",
    "and having n_pixels = 8*4.  The number of possible images to create is k^n_pixels = 4294967296.\n",
    "The training and denoising both have a cyclic complication.\n",
    "It would be difficult to tell when to stop the restoration because each image - noise\n",
    "would be another true image from the training dataset.  So one would want the training\n",
    "set to be some subset of the 4294967296 which have properties like shapes being more than\n",
    "1 adjacent pixel... could perform dilation on all 4294967296 images and keep only the unique subset,\n",
    "and consider other properties like discard images of mostly all 0s or all 1s... Then the\n",
    "universe of train and test images would be feasible to create... haven't thought this\n",
    "through thoroughly... would need to greatly reduce num_train_timesteps and adapt UNet for very small input shape...\n",
    "\n",
    "   The paper equations are here for convenience:\n",
    "\n",
    "     reverse (where the training happens):\n",
    "       eqn (1)\n",
    "       p(x_{t-1} | x_{t}) = N( x_{t-1}; mu_{theta}(x_{t}, t), sigma_{theta}(x_{t}, t) )\n",
    "     forward:\n",
    "       eqn (2)\n",
    "       q(x_{t} | x_{t-1}) = N( x_{t}; sqrt(1-beta_{t}) * x_{t-1}, beta_{t} * I)\n",
    "\n",
    "     training performed using ELBO:\n",
    "       eqn (3)\n",
    "       L = E_{q}[-log(p(x_{T}))\n",
    "                 - sum over t >=1 to T (log(p_{theta}(x_{t-1}|x_{t}) / q(x_{t}|x_{t-1})))]\n",
    "\n",
    "       alpha_{t} = 1 - beta_{t}\n",
    "\n",
    "       alpha_mean_{t} = product over s = 1 to t (alpha_{s}\n",
    "\n",
    "       eqn (4)\n",
    "       q(x_{t}|x_{0}) = N(x_{t}; sqrt(alpha_mean{t}) * x_{0}, (1 - alpha_mean{t}) * I)\n",
    "\n",
    "     rewritten using Kullback-Leibler Divergence, and grouping of terms in 3 segments:\n",
    "       eqn (5)\n",
    "       L = E_{q}[ DKL(q(x_{T}|x_{0}) || p(x_{T})) <--L_{T}\n",
    "                  + sum over t >=1 to T (\n",
    "                    DKL(q(x_{t-1}|x_{t},x_{0}) || p(x_{t-1}|x_{t})) <--L_{t-1}\n",
    "                    )\n",
    "                  - log(p_{theta}(x_{0}|x_{1}))   <-- L_{0}\n",
    "                ]\n",
    "      \n",
    "       eqn (6)\n",
    "       q(x_{t-1}|x_{t},x_{0} = N( x_{t-1}; mu_est_{t}(x_{t}|x_{0}), beta_est_{t}*I )\n",
    "       eqn (7)\n",
    "          where\n",
    "            mu_est_{t}(x_{t}|x_{0}) =\n",
    "              (sqrt(1-alpha_mean_{t-1})*beta_{t} * x_{0}/(1-alpha_mean_{t})\n",
    "              + sqrt(alpha_t)*(1-alpha_mean_{t}) * x_{t}/(1-alpha_mean_{t})\n",
    "          where\n",
    "            beta_est_{t} = (1-alpha_mean_{t-1}) * beta_{t} / (1-alpha_mean_{t})\n",
    "\n",
    "       where reparameterization has been used to express stochastic functions as gaussians which makes the backpropagation easier.\n",
    "\n",
    "       the DKLs can be calculated with closed form expressions in Rao-Blackwellized fashion.\n",
    "              \n",
    "       the authors fix beta_{t} of forward process, making L_{T} const during training (ignorable).\n",
    "       The L_{t-1} term is simplified to eqn (12) in Sec 3.2\n",
    "           L_{t-1} = E_x0_eps[((beta_{t}^2)/(2 * sigma_{t}^2 * alpha_{t}*(1-alpha_mean_{t-1})))\n",
    "                       * ||(eps - eps_theta*(sqrt(alpha_mean_{t})*x_{0} + sqrt(1-alpha_mean_{t})*eps, t)||^2\n",
    "                     ]\n",
    "\n",
    "           where eps_theta is a function approximator to predict eps from x_{t}, where x_{t} is a noisey image.\n",
    "\n",
    "       Sec 3.2 of paper:\n",
    "       the authors set (sigma_{t})^2 to beta_{t} for generative use, x_{0}~N(0,I)\n",
    "       ** but for other uses, could set (sigma_{t})^2 = beta_est_{t}\n",
    "\n",
    "       in Sect 3.2 last paragraph, the authors state that they train the reverse process\n",
    "       mean function approximator mu_theta to predict the noise, eps.\n",
    "       (they add that one can predict an image x_{0}, but found it led to worse sample quality).\n",
    "\n",
    "The Kullback-Leibler divergence is regularization of the latent space, keeping\n",
    "the posterior distribution close to the prior distribution.  \n",
    "\n",
    "The conjugate property of gaussians also enables re-parameterization which simplies\n",
    "the backpropagation.\n",
    "\n",
    "UNet model, specifically, is used for the mean function approximator mu_theta (which is configured to predict the noisy image).\n",
    "DDPMScheduler holds the alphas indexed by t.\n",
    "The training loss (empirical risk) is calculated as square of difference between noisey\n",
    "image created in reverse process and the noise predicted by UNet model.\n",
    "The gradient of the loss is performed by the optimizer, which updates the model parameters\n",
    "(where parameters are the weights and biases in the Model network layers).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ECBzyzIhVDED",
   "metadata": {
    "id": "ECBzyzIhVDED"
   },
   "source": [
    "--------\n",
    "A nice discussion of pros and cons of GANs, VAEs and Diffusion Models (DMs) and when to use each is [this article](https://pub.towardsai.net/diffusion-models-vs-gans-vs-vaes-comparison-of-deep-generative-models-67ab93e0d9ae)\n",
    "\n",
    "Figure 2 of the article summarizes 3 properties of them in a triangle where proximity to the deep model means the model has those characteristics:\n",
    "```\n",
    "\n",
    "               (high quality)\n",
    "               (  samples   )\n",
    "     GANs                           DMs\n",
    "\n",
    "      (fast)               (mode coverage)\n",
    "      (sampling)           (  diversity  )\n",
    "\n",
    "                   VAEs\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
