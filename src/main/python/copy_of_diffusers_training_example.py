# -*- coding: utf-8 -*-
"""Copy of diffusers_training_example.ipynb

Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1iXXHtz9lNPLXBLGyAVIXym_ab2aiCiHL

NLK: have edited to run a very small subset all the way through to edit for updated
arguments and data directories.  Have temporarily removed the cloud options, but
will enable them again.

# ðŸ¤— Training with Diffusers

In recent months, it has become clear that diffusion models have taken the throne as the state-of-the-art generative models. Here, we will use Hugging Face's brand new [Diffusers](https://github.com/huggingface/diffusers) library to train a simple diffusion model.

## Installing the dependencies

This notebook leverages the [ðŸ¤— Datasets](https://huggingface.co/docs/datasets/index) library to load and preprocess image datasets and the [ðŸ¤— Accelerate](https://huggingface.co/docs/accelerate/index) library to simplify training on any number of GPUs, with features like automatic gradient accumulation and tensorboard logging. Let's install them here:
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install git+https://github.com/huggingface/diffusers.git#egg=diffusers[training]
# !pip install accelerate
# !pip install datasets

"""If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.

To be able to share your model with the community, there are a few more steps to follow.|

First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your **write** token:
"""

#from huggingface_hub import notebook_login

#notebook_login()

"""
Then you need to install Git-LFS to upload your model checkpoints:"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !sudo apt -qq install git-lfs
# !git config --global credential.helper store

"""## Config

For convenience, we define a configuration grouping all the training hyperparameters. This would be similar to the arguments used for a [training script](https://github.com/huggingface/diffusers/tree/main/examples).
Here we choose reasonable defaults for hyperparameters like 
`num_epochs`, `learning_rate`, `lr_warmup_steps`, 
but feel free to adjust them if you train on your own dataset. 
For example, `num_epochs` can be increased to 100 for better visual quality.
"""

# to run a small subset of the data, only 1 epoch, and use a very small iteration in making the samples
# in order to follow the processm edit data directories, etc:
run_small_inspect = True

display_train_sample = False

from dataclasses import dataclass

@dataclass
class TrainingConfig:
    image_size = 128  # the generated image resolution
    train_batch_size = 16
    eval_batch_size = 16  # how many images to sample during evaluation
    num_epochs = 50
    gradient_accumulation_steps = 1
    learning_rate = 1e-4
    lr_warmup_steps = 500
    save_image_epochs = 10
    save_model_epochs = 30
    mixed_precision = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision
    output_dir = 'ddpm-butterflies-128'  # the model name locally and on the HF Hub

    push_to_hub = False  # whether to upload the saved model to the HF Hub
    hub_private_repo = False
    overwrite_output_dir = True  # overwrite the old model when re-running the notebook
    seed = 0

config = TrainingConfig()
if run_small_inspect:
    config.num_epochs = 1

"""## Loading the dataset

We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download our image dataset.

In this case, the [Butterflies dataset](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset) is hosted remotely, but you can load a local [ImageFolder](https://huggingface.co/docs/datasets/v2.0.0/en/image_process#imagefolder) as shown in the commets below.
"""
import os

try:
    data_dir = os.environ["ML_DATASETS_HOME"]
except:
    import tempfile
    from tempfile import TemporaryDirectory
    # colab environment
    # other args: url, folder_in_archive
    data_dir = tempfile.mkdtemp()
    print(f'temp_dataset_dir={data_dir}')

from datasets import load_dataset

config.dataset = "huggan/smithsonian_butterflies_subset"
hf_name = config.dataset

data_dir = os.path.join(data_dir, hf_name)

if (not os.path.exists(data_dir)):
    # butterflies
    dataset = load_dataset(path=config.dataset, split="train", keep_in_memory=False)
    dataset.save_to_disk(data_dir)
else:
    from datasets import Dataset
    #from torch.utils.data import Dataset
    # keep_in_memory = False?
    dataset = Dataset.load_from_disk(data_dir, keep_in_memory=False)

if run_small_inspect:
    # load only 1 batch to check pipeline
    import torch
    import numpy as np
    ind = np.array([i for i in range(0, config.train_batch_size)], dtype=np.int32)
    indices = torch.from_numpy(ind)
    ind = ind + config.train_batch_size
    indices2 = torch.from_numpy(ind)
    dataset_train = torch.utils.data.Subset(dataset, indices)
    #dataset_train_ = torch.utils.data.RandomSampler(dataset, num_samples=config.train_batch_size)
    #dataset_test = torch.utils.data.Subset(dataset, indices2)
    dataset = dataset_train

# Feel free to try other datasets from https://hf.co/huggan/ too!
# Here's is a dataset of flower photos:
# config.dataset = "huggan/flowers-102-categories"
# dataset = load_dataset(config.dataset, split="train")

# Or just load images from a local folder!
# config.dataset = "imagefolder"
# dataset = load_dataset(config.dataset, data_dir="path/to/folder")

"""The dataset contains several extra `features` (columns), but the one that we're interested in is `image`:"""

'''
Dataset({
    features: ['image_url', 'image_alt', 'id', 'name', 'scientific_name', 'gender', 'taxonomy', 'region', 'locality', 
    'date', 'usnm_no', 'guid', 'edan_url', 'source', 'stage', 'image', 'image_hash', 'sim_score'],
    num_rows: 1000
})
'''
dataset

"""Since the [`Image`](https://huggingface.co/docs/datasets/image_process#image-datasets) feature loads the images with PIL, we can easily look at a few examples:"""

import matplotlib.pyplot as plt

if display_train_sample:
    fig, axs = plt.subplots(1, 4, figsize=(16, 4))
    for i, image in enumerate(dataset[:4]["image"]):
        axs[i].imshow(image)
        axs[i].set_axis_off()
    fig.show()
else:
    print('skipping plot figures this time')

"""The images in the dataset are all different, so we need to preprocess them first:
* `Resize` makes the images conform to a square resolution of `config.image_size`
* `RandomHorizontalFlip` augments the dataset by randomly mirroring the images.
* `Normalize` is important to rescale the pixel values into a `[-1, 1]` range (which our model will expect).
"""
#!pip3 install torchvision
from torchvision import transforms

preprocess = transforms.Compose(
    [
        transforms.Resize((config.image_size, config.image_size)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5]),
    ]
)

"""ðŸ¤— Datasets offer a handy `set_transform()` method to apply the image transformations on the fly during training:"""

def transform(examples):
    images = [preprocess(image.convert("RGB")) for image in examples["image"]]
    return {"images": images}

try:
    dataset.set_transform(transform)
except:
    #for Subset
    dataset.dataset.set_transform(transform)

"""Let's see what they look like now"""

if display_train_sample:
    fig, axs = plt.subplots(1, 4, figsize=(16, 4))
    for i, image in enumerate(dataset[:4]["images"]):
        axs[i].imshow(image.permute(1, 2, 0).numpy() / 2 + 0.5)
        axs[i].set_axis_off()
    fig.show()
else:
    print('skipping plot figures this time')

"""Now that all our images have the same size and are converted to tensors, we can create the dataloader we will use for training."""

import torch

train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)

"""## Defining the diffusion model

Here we set up our diffusion model. Diffusion models are neural networks that are trained to predict slightly less noisy images from a noisy input. At inference, they can be used to iteratively transform a random noise to generate an image:

<p align="center">
    <img src="https://user-images.githubusercontent.com/10695622/174349667-04e9e485-793b-429a-affe-096e8199ad5b.png" width="800"/>
    <br>
    <em> Figure from DDPM paper (https://arxiv.org/abs/2006.11239). </em>
<p>

Don't worry too much about the math if you're not familiar with it, the import part to remember is that our model corresponds to the arrow $p_{\theta}(x_{t-1}|x_{t})$ (which is a fancy way of saying: predict a slightly less noisy image).

The interesting part is that it's really easy to add some noise to an image, so the training can happen in a semi-supervised fashion as follows:
1. Take an image from the training set.
2. Apply to it some random noise $t$ times (this will give the $x_{t-1}$ and the $x_{t}$ in the figure above).
3. Give this noisy image to the model along with the value of $t$.
4. Compute a loss from the output of the model and the noised image $x_{t-1}$.

Then we can apply gradient descent and repeat this process multiple times.

Most diffusion models use architectures that are some variant of a [U-net](https://arxiv.org/abs/1505.04597) and that's what we'll use here.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/unet-model.png)

In a nutshell:
- the model has the input image go through several blocks of ResNet layers which halves the image size by 2
- then through the same number of blocks that upsample it again.
- there are skip connections linking the features on the downample path to the corresponsding layers in the upsample path.

A key feature of this model is that it predicts images of the same size as the input, which is exactly what we need here.

Diffusers provides us a handy `UNet2DModel` class which creates the desired architecture in PyTorch.

Let's create a U-net for our desired image size.
Note that `down_block_types` correspond to the downsampling blocks (green on the diagram above), and `up_block_types` are the upsampling blocks (red on the diagram):
"""

from diffusers import UNet2DModel

# documentation https://huggingface.co/docs/diffusers/api/models/unet2d
model = UNet2DModel(
    sample_size=config.image_size,  # the target image resolution
    in_channels=3,  # the number of input channels, 3 for RGB images
    out_channels=3,  # the number of output channels
    layers_per_block=2,  # how many ResNet layers to use per UNet block
    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channes for each UNet block
    down_block_types=(
        "DownBlock2D",  # a regular ResNet downsampling block
        "DownBlock2D",
        "DownBlock2D",
        "DownBlock2D",
        "AttnDownBlock2D",  # a ResNet downsampling block with spatial self-attention
        "DownBlock2D",
    ),
    up_block_types=(
        "UpBlock2D",  # a regular ResNet upsampling block
        "AttnUpBlock2D",  # a ResNet upsampling block with spatial self-attention
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D"
      ),
)

"""Let's get a sample image from our dataset and pass it into our model. We just need to add a batch dimension:"""

try:
    sample_image = dataset[0]['images'].unsqueeze(0)
except:
    #Subset
    sample_image = dataset.dataset[0]['images'].unsqueeze(0)
print('Input shape:', sample_image.shape)

"""And let's check the output is a tensor of the same exact shape:"""

print('Output shape:', model(sample_image, timestep=0)["sample"].shape, flush=True)

"""Great!

Note that our model takes in the (noisy) image and also the current time-step (as we saw before in the training overview). That time-step information is converted for the model using a sinusoidal positional embedding, similar to what Transformer models often do.

Now that we have our model, we just need an object to *add noise to an image*. This is done by the **schedulers** in the `diffusers` library.

## Defining the noise scheduler

Depending on the diffusion algorithm you want to use, the way images are noised is slightly different. That's why ðŸ¤— Diffusers contains different scheduler classes which each define the algorithm-specific diffusion steps. Here we are going to use the `DDPMScheduler` which corresponds to the training denoising and training algorithm proposed in [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239).
"""

from diffusers import DDPMScheduler

#noise_scheduler = DDPMScheduler(num_train_timesteps=1000, tensor_format="pt")
noise_scheduler = DDPMScheduler()
noise_scheduler.config.num_train_timesteps = 1000

"""Let's see how this noise scheduler works: it takes a batch of images from the trainng set (here we will reuse the 
batch of one image `sample_image` form before), a batch of random noise of the same shape 
and the timesteps for each image (which correspond to the number of times we want to apply noise to each image):"""

import torch
from PIL import Image

noise = torch.randn(sample_image.shape)
timesteps = torch.LongTensor([50])
noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)

Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])

"""In the DDPM algorithm, the training objective of the model is then to be able to predict the noise we used in `noise_scheduler.add_noise`, so the loss at this step would be:"""

import torch.nn.functional as F

noise_pred = model(noisy_image, timesteps)["sample"]
loss = F.mse_loss(noise_pred, noise)

"""## Setting up training

We have all we need to be able to train our model! Let's use a standard AdamW optimizer:
"""

optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)

""" And a cosine learning rate schedule:"""

from diffusers.optimization import get_cosine_schedule_with_warmup

if run_small_inspect:
    # I change train loop to change lr step only once per epoch
    lr_scheduler = get_cosine_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=config.lr_warmup_steps,
        num_training_steps=(config.num_epochs),
    )
else:
    #TODO: edit this when re-enable use of Accelerator
    lr_scheduler = get_cosine_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=config.lr_warmup_steps,
        #num_training_steps=(len(train_dataloader) * config.num_epochs),
        num_training_steps=(config.num_epochs),
    )


"""To evaluate our model, we use the `DDPMPipeline` which is an easy way to perform end-to-end inference (see this notebook [TODO link] for more detail). We will use this pipeline to generate a batch of sample images and save it as a grid to the disk."""

from diffusers import DDPMPipeline

import math

def make_grid(images, rows, cols):
    w, h = images[0].size
    grid = Image.new('RGB', size=(cols*w, rows*h))
    for i, image in enumerate(images):
        grid.paste(image, box=(i%cols*w, i//cols*h))
    return grid

def evaluate(config, epoch, pipeline):
    # Sample some images from random noise (this is the backward diffusion process).
    # The default pipeline output type is `List[PIL.Image]`
    print(f'evaluate {epoch}')
    # default num_inference_steps=1000

    if run_small_inspect:
        result = pipeline(
            batch_size = config.eval_batch_size,
            generator=torch.manual_seed(config.seed),
            num_inference_steps=2,
        )
    else:
        result = pipeline(
            batch_size = config.eval_batch_size,
            generator=torch.manual_seed(config.seed),
        )

    # result type is ImagePipelineOutput
    #print(f'result={result}')
    images = result["images"]

    # Make a grid out of the images
    image_grid = make_grid(images, rows=4, cols=4)

    # Save the images
    print('save sampled images to disk')
    test_dir = os.path.join(config.output_dir, "samples")
    os.makedirs(test_dir, exist_ok=True)
    image_grid.save(f"{test_dir}/{epoch:04d}.png")

"""With this in end, we can group all together and write our training function. 
This just wraps the training step we saw in the previous section in a loop, 
using Accelerate for easy TensorBoard logging, gradient accumulation, 
mixed precision training and multi-GPUs or TPU training."""

# https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html

from accelerate import Accelerator
import diffusers
#from diffusers.utils.hub_utils import init_git_repo, push_to_hub

#from tqdm.auto import tqdm
import os
'''
def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):
    # Initialize accelerator and tensorboard logging
    accelerator = Accelerator(
        mixed_precision=config.mixed_precision,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        log_with="tensorboard",
        #logging_dir=os.path.join(config.output_dir, "logs")
        project_dir=os.path.join(config.output_dir, "logs")
    )
    if accelerator.is_main_process:
        #if config.push_to_hub:
        #    repo = init_git_repo(config, at_init=True)
        accelerator.init_trackers("train_example")

    # Prepare everything
    # There is no specific order to remember, you just need to unpack the
    # objects in the same order you gave them to the prepare method.
    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, lr_scheduler
    )

    global_step = 0

    # Now you train the model
    for epoch in range(config.num_epochs):
        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)
        progress_bar.set_description(f"Epoch {epoch}")

        for step, batch in enumerate(train_dataloader):
            clean_images = batch['images']
            # Sample noise to add to the images
            noise = torch.randn(clean_images.shape).to(clean_images.device)
            bs = clean_images.shape[0]

            # Sample a random timestep for each image
            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()

            # Add noise to the clean images according to the noise magnitude at each timestep
            # (this is the forward diffusion process)
            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)

            with accelerator.accumulate(model):
                # Predict the noise residual
                noise_pred = model(noisy_images, timesteps)["sample"]
                loss = F.mse_loss(noise_pred, noise)
                accelerator.backward(loss)

                accelerator.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            progress_bar.update(1)
            logs = {"loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0], "step": global_step}
            progress_bar.set_postfix(**logs)
            accelerator.log(logs, step=global_step)
            global_step += 1

        # After each epoch you optionally sample some demo images with evaluate() and save the model
        if accelerator.is_main_process:
            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)

            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:
                evaluate(config, epoch, pipeline)

            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:
                #if config.push_to_hub:
                #    push_to_hub(config, pipeline, repo, commit_message=f"Epoch {epoch}", blocking=True)
                #else:
                #    pipeline.save_pretrained(config.output_dir)
                pipeline.save_pretrained(config.output_dir)
'''

def train_loop_no_accelerator(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):
    print('start train loop')
    #config.mixed_precision,
    #config.gradient_accumulation_steps,

    size = len(train_dataloader.dataset)
    model.train()  # sets the module to training mode

    global_step = 0

    # Now you train the model
    for epoch in range(config.num_epochs):

        for step, batch in enumerate(train_dataloader):
            clean_images = batch['images'] #(16,3,128,128)
            # Sample noise to add to the images
            noise = torch.randn(clean_images.shape).to(clean_images.device)
            bs = clean_images.shape[0]

            # Sample a random timestep for each image
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device).long()

            # Add noise to the clean images according to the noise magnitude at each timestep
            # (this is the forward diffusion process)
            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)

            # Predict the noise residual
            noise_pred = model(noisy_images, timesteps)["sample"]
            loss = F.mse_loss(noise_pred, noise)

            # Backpropagation
            if run_small_inspect:
                print('backpropagation', flush=True)
            loss.backward()
            if run_small_inspect:
                print('update params', flush=True)
            optimizer.step()
            optimizer.zero_grad()

            if run_small_inspect or step % 100 == 0:
                loss, current = loss.detach().item(), (step + 1) * len(clean_images)
                print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}, {global_step}]")

            global_step += 1

        # after an epoch of train and eval, step scheduler:
        lr_scheduler.step()

        # After each epoch you optionally sample some demo images with evaluate() and save the model
        if True:
            print('constuct pipeline')
            # pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)
            pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)

            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:
                # this takes a long time:
                print('evaluate pipeline')
                #print('skipping evaluate pipeline for now')
                evaluate(config, epoch, pipeline)

            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:
                # if config.push_to_hub:
                #    push_to_hub(config, pipeline, repo, commit_message=f"Epoch {epoch}", blocking=True)
                # else:
                #    pipeline.save_pretrained(config.output_dir)
                print('save pipeline configuration')
                pipeline.save_pretrained(config.output_dir)


"""## Let's train!

Let's launch the training (including multi-GPU training) from the notebook using Accelerate's `notebook_launcher` function:
"""

from accelerate import notebook_launcher
args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)

#notebook_launcher(train_loop, args, num_processes=1)
train_loop_no_accelerator(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)
print('done training')

"""Let's have a look at the final image grid produced by the trained diffusion model:"""

import glob

sample_images = sorted(glob.glob(f"{config.output_dir}/samples/*.png"))
Image.open(sample_images[-1])

print('done with display sample of images')

"""Not bad! There's room for improvement of course, so feel free to play with the hyperparameters, model definition and image augmentations ðŸ¤—

If you've chosen to upload the model to the Hugging Face Hub, its repository should now look like so:
https://huggingface.co/anton-l/ddpm-butterflies-128

If you want to dive deeper into the code, we also have more advanced training scripts with features like Exponential Moving Average of model weights here:

https://github.com/huggingface/diffusers/tree/main/examples
"""

