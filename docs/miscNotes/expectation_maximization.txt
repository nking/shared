Maximum Likelihood from Incomplete Data via the EM Algorithm
A. P. Dempster; N. M. Laird; D. B. Rubin
Journal of the Royal Statistical Society. Series B (Methodological), Vol. 39, No. 1. (1977), pp. 1-38.

NOTE: rewriting the variables to use Stanford ML CS229 notation.

x are the observed.
z are not observed and are called the complete data.

postulate a family of sampling densities f (z I θ) depending on parameters phi.  <== complete-data specification.
corresponding family of sampling densities is g(x | θ)  <== incomplete-data specification.
    EQN 1.1:
    g(x | θ) = integral over z of ( f(z | θ) dz)

EM aims to find the value of θ which maximizes g(x | θ) given an observed x, by using f(z | θ)

* there are many possible complete-data specifications f(z | θ) that will generate g(x | θ) 

Simplest Example
Rao (1965, pp. 368-369) presents data in which 197 animals are distributed multinomially 
into four categories, so that the observed data consist of 
    x = (x1, x2, x3, x4) = (125, 18, 20, 34)

A genetic model for the population specifies cell probabilities
      p(θ;x) = [ (0.5 + 0.25*θ), (0.25*(1 - θ)), (0.25*(1 - θ)), (0.25*θ)] for θ in [0,1]
      NOTE: the notation p(θ;x) is mine, to distinguish between p(θ) used in g(x | θ) and f(z | θ).

EQN 1.2:  g(x | θ) = ( (x1 + x2 + x3 + x4)!/( x1! * x2! * x3! * x4!) ) 
                        * p1(θ;x) ^x1* p2(θ;x) ^x2 * p3(θ;x) ^x3 * p4(θ;x) ^x4

Rao uses θ = (1 - psi)^2 to take 1 step of Fisher scoring procedure for max g(x | (1-psi)^2) given the observed x.

x is incomplete data from a 5-category multinomial population with cell probabilities:
         p(θ; z) = [ (0.5),  (0.25*θ), (0.25*(1 - θ)), (0.25*(1 - θ)), (0.25*θ) ] for θ in [0,1] where p1(θ; x) is split into 2 for z

Now, the complete data are 
    z = (z1, z2, z3, z4, z5)  where x1 = z1 + z2,  x2 = z3, x3 = z4, x4 = z5 
  => z = (z1, x1-z1, x2, x3, x4)
and the complete data specification is:
    EQN 1.3:  f(z | θ) = ( (z1 + z2 + z3 + z4 + z5)!/( z1! * z2! * z3! * z4! * z5!) ) 
                        * p1(θ;z)^z1 *p2(θ;z)^z2 * p3(θ;z)^z3 * p4(θ;z)^z4 * p5(θ;z)^z5

The expectation step:
Note that the integral in (1.1)  
     which is integral over z of ( f(z | θ) dz)
consists in this case of summing (1.3) 
     which is f(z | θ) = ( (z1 + z2 + z3 + z4 + z5)!/( z1! * z2! * z3! * z4! * z5!) ) 
                        * p1(θ;z)^z1 * p2(θ;z)^z2 * p3(θ;z)^z3 * p4(θ;z)^z4 * p5(θ;z)^z5
over the ( zi, zj pairs (0,125), (1,124), ...,(125, O), while simply substituting (18,20,34) for (z3, z4, z5).
where the observed data x = (x1, x2, x3, x4) = (125, 18, 20, 34) so that the only sufficient statistics that need to be
estimated are z1 and z2 where z1 + z2 = x1 = 125.
    using the current θ, we estimate EQNs 1.4:
   z1^(m) = (z1 + z2) * ( p1(θ;z)/(p1(θ;z) + p2(θ;z)) = 125 * ( p1(θ;z)/(p1(θ;z) + p2(θ;z))
   and
   z2^(m) = (z1 + z2) * ( p2(θ;z)/(p1(θ;z) + p2(θ;z)) = 125 * ( p2(θ;z)/(p1(θ;z) + p2(θ;z))
The maximization step then takes the estimated complete data (z1^(m),z2^(m), 18,20,34) and estimates θ by maximum likelihood as though the estimated complete data were the observed data. 
setting d/dθ(log(f(z|θ))) to 0 leads to:
EQN 1.5 θ^(m+1) = (z2^(m) + z5)/(z2^(m) + z5 + z3 + z4) = (z2^(m) + 34)/(z2^(m) + 34 + 18 + 20) 

For this example, EM cycles between EQNs 1.4 and 1.5.
initialize θ = 0.5

python code:

x=[125, 18, 20, 34]
z=[0,0,x[1],x[2],x[3]]

def calcz0(t): ‘’’ eqn 1.4 of Dempster, Laird, and Rubin 1976’’’
    return x[0] * (0.5)/(.5+0.25*t)

def calcz1(t): ‘’’ eqn 1.4 of Dempster, Laird, and Rubin 1976’’’
    return x[0] * (0.25*t)/(.5+0.25*t)

def calct(z):‘’’ eqn 1.5 of Dempster, Laird, and Rubin 1976’’’
    return (z[1] + z[4])/(z[1] + z[4] + z[2] + z[3])

t=0.5
tp=0
while ((t-tp) > 1E-7):
    tp = t
    z[0] = calcz0(t)
    z[1] = calcz1(t)
    t = calct(z)
    t
compare to Table 1 of paper
