<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>PrincipalComponents.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Jacoco Report</a> &gt; <a href="index.source.html" class="el_package">algorithms.dimensionReduction</a> &gt; <span class="el_source">PrincipalComponents.java</span></div><h1>PrincipalComponents.java</h1><pre class="source lang-java linenums">package algorithms.dimensionReduction;

import algorithms.correlation.BruteForce;
import algorithms.matrix.MatrixUtil;
import java.util.Arrays;
import no.uib.cipr.matrix.DenseMatrix;
import no.uib.cipr.matrix.Matrices;
import no.uib.cipr.matrix.NotConvergedException;
import no.uib.cipr.matrix.SVD;

/**
 * find the principal components, that is, the minimal residual variance bases
 * of vectors of samples following G. Strang's SVD in machine learning in
 * http://www.cs.toronto.edu/~jepson/csc420/
 * 
 * @author nichole
 */
<span class="pc bpc" id="L18" title="1 of 2 branches missed.">public class PrincipalComponents {</span>
    
    /**
     * calculate the principal components of the unit standardized data x
     * using SVD.
     * NOTE: should standardize the data before using this method,
     * &lt;pre&gt;
     *      double[] mean = new double[data[0].length];
            double[] stDev = new double[data[0].length];
     * e.g. double[][] x = Standardization.standardUnitNormalization(data, mean, stDev);
     * &lt;/pre&gt;
     * 
     * from http://www.cs.toronto.edu/~jepson/csc420/
     * combined with the book by Strang &quot;Introduction to Linear Algebra&quot; 
     * and the book by Leskovec, Rajaraman, and Ullman &quot;Mining of Massive Datasets&quot;.
     * also useful:
     * https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca
     * and https://online.stat.psu.edu/stat505/book/export/html/670
     * 
     * NOTE: SVD has the drawbacks that a singular vector specifies a linear
       combination of all input columns or rows, and there is a
       a Lack of sparsity in the U, V, and s matrices.
       See CUR decomposition method in contrast.
       http://www.mmds.org/mmds/v2.1/ch11-dimred.pdf
     * @param x is a 2-dimensional array of k vectors of length n in format
     *    double[n][k].  n is the number of samples, and k is the number of
     *    variables, a.k.a. dimensions.
     * @param nComponents the number of principal components to return.
     * @return a few statistics of the SVD of the covariance of A, up to the
     * nComponents dimension.  Note that if the rank of the SVD(cov(A)) is
     * less than nComponents, then only that amount is returned.
     */
    public static PCAStats calcPrincipalComponents(double[][] x, int nComponents) throws NotConvergedException {
                
<span class="fc" id="L52">        int n = x.length;</span>
<span class="fc" id="L53">        int nDimensions = x[0].length;</span>
        
<span class="pc bpc" id="L55" title="1 of 2 branches missed.">        if (nComponents &gt; nDimensions) {</span>
<span class="nc" id="L56">            throw new IllegalArgumentException(&quot;x has &quot; + nDimensions + &quot; but &quot; </span>
                + &quot; nComponents principal components were requested.&quot;);
        }
        
<span class="fc" id="L60">        double eps = 1.e-15;</span>
           
        /*
        NOTE that the Strang approach does not assume the data has been
        standardized to mean 0 and unit variance.
        
        minimal residual variance basis:
           basis direction b.
           samples are vectors x_j where j = 0 to k
           sample mean m_s = (1/k) summation_over_j(x_j)

           looking for a p-dimensional basis that minimizes:
              SSD_p = min_{B}( summation_over_j( min_{a_j}(||x_j - (m_s + B*a_j)||^2) ) )
                 where B = (b1, . . . , bp) is the n × p matrix formed from the selected basis.

              choose the coefficients ⃗aj which minimize the least squares error
                 E_j^2 = ||x_j - (m_s + B*a_j)||^2
              then choose B to minimize the SSD = summation_over_j(E_j^2)

           SSD_p is called the minimum residual variance for any basis of dimension p.

           [U, S, V] = SVD( Cov )
           the 1st principal direction is the 1st column of U.
        */
    
<span class="fc" id="L85">        double[][] cov = BruteForce.covariance(x);</span>
<span class="pc bpc" id="L86" title="3 of 4 branches missed.">        assert(nDimensions == cov.length);</span>
<span class="pc bpc" id="L87" title="3 of 4 branches missed.">        assert(nDimensions == cov[0].length);</span>
                        
        //NOTE: we know that cov is symmetric positive definite, so there are
        //   many operations special to it one could explore for diagonalization
        
<span class="fc" id="L92">        SVD svd = SVD.factorize(new DenseMatrix(cov));</span>
        // U is mxm
        // S is mxn
        // V is nxn
        
<span class="fc" id="L97">        double[] s = svd.getS();</span>
        
<span class="fc" id="L99">        int rank = 0;</span>
<span class="fc bfc" id="L100" title="All 2 branches covered.">        for (int i = 0; i &lt; s.length; ++i) {</span>
<span class="pc bpc" id="L101" title="1 of 2 branches missed.">            if (Math.abs(s[i]) &gt; eps) {</span>
<span class="fc" id="L102">                rank++;</span>
            }
        }
        
<span class="pc bpc" id="L106" title="1 of 2 branches missed.">        if (nComponents &gt; rank) {</span>
<span class="nc" id="L107">            nComponents = rank;</span>
        }
        
        //eigenvalue_i = lambda_i = (s_i^2)/(n-1)
<span class="fc" id="L111">        double[] eV = Arrays.copyOf(s, rank);</span>
<span class="fc bfc" id="L112" title="All 2 branches covered.">        for (int i = 0; i &lt; eV.length; ++i) {</span>
<span class="fc" id="L113">            eV[i] *= eV[i];</span>
<span class="fc" id="L114">            eV[i] /= ((double)n - 1.);</span>
        }
        
        // COLUMNS of u are the principal axes, a.k.a. principal directions
        // size is nDimensions x nDimensions
<span class="fc" id="L119">        DenseMatrix u = svd.getU();</span>
<span class="pc bpc" id="L120" title="3 of 4 branches missed.">        assert(nDimensions == u.numRows());</span>
<span class="pc bpc" id="L121" title="3 of 4 branches missed.">        assert(nDimensions == u.numColumns());</span>
                     
        // extract the nComponents columns of U as the principal axes, a.k.a. 
        //  principal directions.  array format: [nDimensions][nComponents]
<span class="fc" id="L125">        double[][] pa = new double[u.numRows()][nComponents];                                </span>
<span class="fc bfc" id="L126" title="All 2 branches covered.">        for (int row = 0; row &lt; u.numRows(); ++row) {</span>
<span class="fc" id="L127">            pa[row] = new double[nComponents];</span>
<span class="fc bfc" id="L128" title="All 2 branches covered.">            for (int col = 0; col &lt; nComponents; ++col) {</span>
<span class="fc" id="L129">                pa[row][col] = u.get(row, col);</span>
            }
        }
        
        // NOTE: the principal scores is a vector Y_1 of length n:
        //     first principal score Y_1 = 
        //         sum of (first principal direction for each variable dot x[*][variable] )
        //  The magnitudes of the principal direction coefficients give the 
        //  contributions of each variable to that component. 
        //  Because the data have been standardized, they do not depend on the 
        //  variances of the corresponding variables.      
        
        // principal components are the projections of the principal axes on U
        //   the &quot;1 sigma&quot; lengths are:
<span class="fc" id="L143">        double[][] projections = new double[pa.length][pa[0].length];</span>
<span class="fc bfc" id="L144" title="All 2 branches covered.">        for (int row = 0; row &lt; pa.length; ++row) {</span>
<span class="fc" id="L145">            projections[row] = Arrays.copyOf(pa[row], pa[row].length);</span>
<span class="fc bfc" id="L146" title="All 2 branches covered.">            for (int col = 0; col &lt; pa[row].length; ++col) {</span>
                // note, if wanted &quot;3 sigma&quot; instead, use factor 3*s[col] here:
<span class="fc" id="L148">                projections[row][col] *= s[col];</span>
            }
        }
        
        // extract the first nComponents of rows of V^T:
<span class="fc" id="L153">        double[][] v = MatrixUtil.convertToRowMajor(svd.getVt());</span>
<span class="fc" id="L154">        v = MatrixUtil.copySubMatrix(v, 0, nComponents-1, 0, v[0].length-1);</span>
        
<span class="fc" id="L156">        PCAStats stats = new PCAStats();</span>
<span class="fc" id="L157">        stats.nComponents = nComponents;</span>
<span class="fc" id="L158">        stats.principalDirections = pa;</span>
<span class="fc" id="L159">        stats.eigenvalues = eV;</span>
<span class="fc" id="L160">        stats.vTP = v;</span>
        
<span class="fc" id="L162">        stats.s = new double[nComponents];</span>
<span class="fc bfc" id="L163" title="All 2 branches covered.">        for (int i = 0; i &lt; nComponents; ++i) {</span>
<span class="fc" id="L164">            stats.s[i] = s[i];</span>
        }
                
<span class="fc" id="L167">        double total = 0;</span>
<span class="fc bfc" id="L168" title="All 2 branches covered.">        for (int j = 0; j &lt; rank; ++j) {</span>
<span class="fc" id="L169">            total += s[j];</span>
        }
<span class="fc" id="L171">        double[] fracs = new double[rank];</span>
<span class="fc bfc" id="L172" title="All 2 branches covered.">        for (int j = 0; j &lt; rank; ++j) {</span>
<span class="fc" id="L173">            fracs[j] = s[j]/total;</span>
        }
<span class="fc" id="L175">        double[] c = Arrays.copyOf(fracs, fracs.length);</span>
<span class="fc bfc" id="L176" title="All 2 branches covered.">        for (int j = 1; j &lt; c.length; ++j) {</span>
<span class="fc" id="L177">            c[j] += c[j-1];</span>
        }
<span class="fc" id="L179">        stats.cumulativeProportion = c;</span>
        
<span class="fc" id="L181">        double sum = 0;</span>
        int p;
<span class="fc bfc" id="L183" title="All 2 branches covered.">        for (int j = (nComponents+1); j &lt;= nDimensions; ++j) {</span>
<span class="fc" id="L184">            p = j - 1;</span>
<span class="fc" id="L185">            sum += s[p];</span>
        }
<span class="fc" id="L187">        stats.ssdP = sum;</span>
<span class="fc" id="L188">        stats.fractionVariance = (total - sum)/total;</span>
        
<span class="fc" id="L190">        System.out.println(&quot;singular values of SVD(cov) = sqrts of eigenvalues of cov = &quot;);</span>
<span class="fc bfc" id="L191" title="All 2 branches covered.">        for (int i = 0; i &lt; rank; ++i) {</span>
<span class="fc" id="L192">            System.out.printf(&quot;%11.3e  &quot;, s[i]);</span>
        }
<span class="fc" id="L194">        System.out.println();</span>
<span class="fc" id="L195">        System.out.println(&quot;eigenvalues of cov = &quot;);</span>
<span class="fc bfc" id="L196" title="All 2 branches covered.">        for (int i = 0; i &lt; eV.length; ++i) {</span>
<span class="fc" id="L197">            System.out.printf(&quot;%11.3e  &quot;, eV[i]);</span>
        }
<span class="fc" id="L199">        System.out.println();</span>
<span class="fc" id="L200">        System.out.println(&quot;U of SVD(cov) = &quot;);</span>
<span class="fc bfc" id="L201" title="All 2 branches covered.">        for (int i = 0; i &lt; u.numRows(); ++i) {</span>
<span class="fc bfc" id="L202" title="All 2 branches covered.">            for (int j = 0; j &lt; u.numColumns(); ++j) {</span>
<span class="fc" id="L203">                System.out.printf(&quot;%12.5e  &quot;, u.get(i, j));</span>
            }
<span class="fc" id="L205">            System.out.printf(&quot;\n&quot;);</span>
        }
<span class="fc" id="L207">        System.out.println(&quot;V_p of SVD(cov) = &quot;);</span>
<span class="fc bfc" id="L208" title="All 2 branches covered.">        for (int i = 0; i &lt; v.length; ++i) {</span>
<span class="fc bfc" id="L209" title="All 2 branches covered.">            for (int j = 0; j &lt; v[i].length; ++j) {</span>
<span class="fc" id="L210">                System.out.printf(&quot;%12.5e  &quot;, v[i][j]);</span>
            }
<span class="fc" id="L212">            System.out.printf(&quot;\n&quot;);</span>
        }
        
<span class="fc" id="L215">        System.out.println(&quot;eigenvalue fractions of total = &quot;);</span>
<span class="fc bfc" id="L216" title="All 2 branches covered.">        for (int i = 0; i &lt; fracs.length; ++i) {</span>
<span class="fc" id="L217">            System.out.printf(&quot;%11.3e  &quot;, fracs[i]);</span>
        }
<span class="fc" id="L219">        System.out.println();</span>
<span class="fc" id="L220">        System.out.println(&quot;eigenvalue cumulativeProportion= &quot;);</span>
<span class="fc bfc" id="L221" title="All 2 branches covered.">        for (int i = 0; i &lt; stats.cumulativeProportion.length; ++i) {</span>
<span class="fc" id="L222">            System.out.printf(&quot;%11.3e  &quot;, stats.cumulativeProportion[i]);</span>
        }
<span class="fc" id="L224">        System.out.println();</span>
        
<span class="fc" id="L226">        System.out.println(&quot;principal directions= &quot;);</span>
<span class="fc bfc" id="L227" title="All 2 branches covered.">        for (int i = 0; i &lt; stats.principalDirections.length; ++i) {</span>
<span class="fc bfc" id="L228" title="All 2 branches covered.">            for (int j = 0; j &lt; stats.principalDirections[i].length; ++j) {</span>
<span class="fc" id="L229">                System.out.printf(&quot;%12.5e  &quot;, stats.principalDirections[i][j]);</span>
            }
<span class="fc" id="L231">            System.out.printf(&quot;\n&quot;);</span>
        }
<span class="fc" id="L233">        System.out.println(&quot;principal projections (=u_p*s)=&quot;);</span>
<span class="fc bfc" id="L234" title="All 2 branches covered.">        for (int i = 0; i &lt; projections.length; ++i) {</span>
<span class="fc bfc" id="L235" title="All 2 branches covered.">            for (int j = 0; j &lt; projections[i].length; ++j) {</span>
<span class="fc" id="L236">                System.out.printf(&quot;%11.3e  &quot;, projections[i][j]);</span>
            }
<span class="fc" id="L238">            System.out.printf(&quot;\n&quot;);</span>
        }
        
<span class="fc" id="L241">        System.out.flush();</span>
        
<span class="fc" id="L243">        return stats;</span>
    }
    
    /**
     * reconstruct an image from x, given principal directions.
     * from http://www.cs.toronto.edu/~jepson/csc420/
     * combined with the book by Strang &quot;Introduction to Linear Algebra&quot; 
     * and the book by Leskovec, Rajaraman, and Ullman &quot;Mining of Massive Datasets&quot;
     * &lt;pre&gt;
     *     ⃗r(⃗a_0) = m⃗_s + U_p * ⃗a 
     *         where U_p is the principalDirections matrix,
     *         where m_s is the sample x mean,
     *         where a_0 is
     *            a_0 = arg min_{a} ||⃗x − (m⃗_s + U _p * a)||^2
     * &lt;/pre&gt;
     * @param x a 2-dimensional array of k vectors of length n in format
     *    double[n][k] which is double[nSamples][nDimensions] for which to apply
     *    the principal axes transformation on.
     * @param stats the statistics of the principal axes derived from
     * SVD of the covariance of the training data.
     * @return 
     */
    public static double[][] reconstruct(double[][] x, PCAStats stats) {
        
        /* find the 'a' which minimizes ||⃗x − (m⃗_s + p * a)||^2
        ⃗
        then the reconstruction is r(⃗a ) = m⃗ + U ⃗a 
        */
       
<span class="fc" id="L272">        double[][] b = MatrixUtil.multiply(x, stats.principalDirections);</span>
<span class="fc" id="L273">        b = MatrixUtil.multiply(b, stats.vTP);</span>
        
<span class="fc" id="L275">        return b;</span>
    }
    
    public static class PCAStats {
        /** the number of components requested from the calculation, that is,
         the first p principal components from the U matrix of the SVD 
         decomposition of the covariance of x.
         * NOTE that x given to the algorithm, is a sample of vectors 
         *    {⃗x_j}_{j=1 to k}, with each ⃗x_j ∈ R^n (each x_j is a vector of 
         *    n real numbers) that form the matrix X ∈ R^(nxk).
. 
         */
        int nComponents;
        
        /**
         * the first nComponents principal directions of x. 
         * this is obtained from the first p columns of the U matrix of SVD(cov(x)).
         * the U_p columns have been transposed into rows here:
         * principalDirections[0] holds 1st principal direction,
         * principalDirections[1] holds 2nd principal direction, etc.
         * &lt;pre&gt;
         *    x * principalDirections * v^T_p gives the principal axes which
         *       minimize the variance.
         * &lt;/pre&gt;
         */
        double[][] principalDirections;
        
        /**
         * the first p rows of the V^T matrix of SVD(cov(x)).
         * &lt;pre&gt;
         *    x * principalDirections * v^T_p gives the principal axes which
         *       minimize the variance.
         * &lt;/pre&gt;
         */
        double[][] vTP;
        
        /**
         * the fraction of the total variance Q_p (where p is the dimension
         * number, that is, nComponents):
         * &lt;pre&gt;
         *    Q_p = (SSD_0 − SSD_p)/SSD_0
         * 
         *    where SSD_p = summation_{j=p+1 to n}(s_j)
         *       where s_j is from the diagonal matrix S of the SVD
         *          of the covariance of x.
         * &lt;/pre&gt;
         */
        double fractionVariance;
        
        /**
         * the cumulative addition of 
         *     (sum_of_eigenvalues - eigenvalue_i)/sum_of_eigeneigenvalues
         */
        double[] cumulativeProportion;
        
        /**
         * Minimum residual variance
         */
        double ssdP;
        
        /**
         * the first nComponents singular values from the diagonal matrix of
         * the SVD of covariance of x;
         */
        double[] s;
        double[] eigenvalues;
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.9.201702052155</span></div></body></html>