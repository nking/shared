<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>PrincipalComponents.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Jacoco Report</a> &gt; <a href="index.source.html" class="el_package">algorithms.dimensionReduction</a> &gt; <span class="el_source">PrincipalComponents.java</span></div><h1>PrincipalComponents.java</h1><pre class="source lang-java linenums">package algorithms.dimensionReduction;

import algorithms.correlation.BruteForce;
import algorithms.matrix.MatrixUtil;
import java.util.Arrays;

import algorithms.statistics.Covariance;
import algorithms.statistics.Standardization;
import algorithms.util.FormatArray;
import no.uib.cipr.matrix.DenseMatrix;
import no.uib.cipr.matrix.NotConvergedException;
import no.uib.cipr.matrix.SVD;

/**
 * find the principal components, that is, the minimal residual variance bases
 * of vectors of samples following G. Strang's SVD in machine learning in
 * http://www.cs.toronto.edu/~jepson/csc420/
 * 
 * from wikipedia:
 * The principal components of a collection of points in a real coordinate space 
 * are a sequence of p unit vectors, where the i-th vector is the direction of 
 * a line that best fits the data while being orthogonal to the first i-1 vectors. 
 * Here, a best-fitting line is defined as one that minimizes the average 
 * squared distance from the points to the line. These directions constitute an 
 * orthonormal basis in which different individual dimensions of the data are 
 * linearly uncorrelated. Principal component analysis (PCA) is the process of 
 * computing the principal components and using them to perform a change of basis 
 * on the data, sometimes using only the first few principal components and 
 * ignoring the rest.
 * 
 * from wikipedia:
 * 
 * PCA is closely related to Fisher's Discriminant Anaylsisi a.k.a. 
 * Linear Discriminant analysis and factor analysis in that they both look for 
 * linear combinations of variables which best explain the data.[4] 
 * LDA explicitly attempts to model the difference between the classes of data. 
 * PCA, in contrast, does not take into account any difference in class, 
 * and factor analysis builds the feature combinations based on differences 
 * rather than similarities.
 * NOTE:  Fisher's original article[1] actually describes a slightly different 
 * discriminant, which does not make some of the assumptions of LDA such as 
 * normally distributed classes or equal class covariances.
 * 
 * LDA is closely related to analysis of variance (ANOVA) and regression analysis, 
 * which also attempt to express one dependent variable as a linear combination 
 * of other features or measurements.[1][2] However, ANOVA uses categorical 
 * independent variables and a continuous dependent variable, whereas 
 * discriminant analysis has continuous independent variables and a categorical 
 * dependent variable (i.e. the class label).[3] Logistic regression and 
 * probit regression are more similar to LDA than ANOVA is, as they also 
 * explain a categorical variable by the values of continuous independent 
 * variables. These other methods are preferable in applications where it is 
 * not reasonable to assume that the independent variables are normally 
 * distributed, which is a fundamental assumption of the LDA method.
 * 
 * @author nichole
 */
<span class="nc" id="L58">public class PrincipalComponents {</span>

    /**
     * calculate the principal components of the unit standardized data x
     * using Singular Value Decomposition.
     * NOTE: the data need to be zero centered first.
     *
     &lt;pre&gt;
     the method follows:
     http://www.cs.toronto.edu/~jepson/csc420/
     combined with the book by Strang &quot;Introduction to Linear Algebra&quot;
     also useful:
     https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca
     and https://online.stat.psu.edu/stat505/book/export/html/670 for testing results,
     &lt;/pre&gt;
     &lt;pre&gt;
     NOTE:
     variance-covariance(standardized data) == correlation(unstandardized data) == correlation(zero mean centered data).
     therefore, pca using the standardized data == pca using the correlation matrix.
     Also, the eigen of cov(standardized) == eigen of cor(unstandardized).
     &lt;/pre&gt;
     * @param x is a 2-dimensional array of k vectors of length n in format
     *    double[n][k].  n is the number of samples, and k is the number of
     *    variables, a.k.a. dimensions.
     *    x should be zero-centered (mean=0) OR standardized to unit normalization (which is mean=0, stdev=1).
     *    If the variance and scale of the variables are different, then unit standard normalization should be used.
     @param nComponents the number of principal components to return.
     @return the principal axes, the principal components, and
     a few statistics of the CUR decomposition of A, up to the
     nComponents dimension.  Note that if the rank of the A is
     less than nComponents, then only the number of components as rank is returned.
     */
    public static PCAStats calcPrincipalComponents(double[][] x, int nComponents) throws NotConvergedException {
<span class="fc" id="L91">        return calcPrincipalComponents(x, nComponents, true);</span>
    }

    /**
     * calculate the principal components of the unit standardized data x
     * using Singular Value Decomposition or CUR Decomposition.
     * NOTE: the data need to be zero centered first.
     *
     &lt;pre&gt;
     the method follows:
     http://www.cs.toronto.edu/~jepson/csc420/
     combined with the book by Strang &quot;Introduction to Linear Algebra&quot;
     and the book by Leskovec, Rajaraman, and Ullman &quot;Mining of Massive Datasets&quot;.
     also useful:
     https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca
     and https://online.stat.psu.edu/stat505/book/export/html/670
     &lt;/pre&gt;
     &lt;pre&gt;
     NOTE:
     variance-covariance(standardized data) == correlation(unstandardized data) == correlation(zero mean centered data).
     therefore, pca using the standardized data == pca using the correlation matrix.
     Also, the eigen of cov(standardized) == eigen of cor(unstandardized).
     &lt;/pre&gt;
     * @param x is a 2-dimensional array of k vectors of length n in format
     *    double[n][k].  n is the number of samples, and k is the number of
     *    variables, a.k.a. dimensions.
     *    x should be zero-centered (mean=0) OR standardized to unit normalization (which is mean=0, stdev=1).
     *    If the variance and scale of the variables are different, then unit standard normalization should be used.
     @param nComponents the number of principal components to return.
     @return the principal axes, the principal components, and
     a few statistics of the CUR decomposition of A, up to the
     nComponents dimension.  Note that if the rank of the A is
     less than nComponents, then only the number of components as rank is returned.
     */
    public static PCAStats calcPrincipalComponents(double[][] x, int nComponents, boolean useSVD) throws NotConvergedException {

<span class="fc" id="L127">        int n = x.length;</span>
<span class="fc" id="L128">        int nDimensions = x[0].length;</span>

<span class="pc bpc" id="L130" title="1 of 2 branches missed.">        if (nComponents &gt; nDimensions) {</span>
<span class="nc" id="L131">            throw new IllegalArgumentException(&quot;x has &quot; + nDimensions + &quot; but &quot;</span>
                    + &quot; nComponents principal components were requested.&quot;);
        }

<span class="fc" id="L135">        double eps = 1.e-15;</span>

        /*
        minimal residual variance basis:
           basis direction b.
           samples are vectors x_j where j = 0 to k
           sample mean m_s = (1/k) summation_over_j(x_j)

           looking for a p-dimensional basis that minimizes:
              SSD_p = min_{B}( summation_over_j( min_{a_j}(||x_j - (m_s + B*a_j)||^2) ) )
                 where B = (b1, . . . , bp) is the n × p matrix formed from the selected basis.

              choose the coefficients ⃗aj which minimize the least squares error
                 E_j^2 = ||x_j - (m_s + B*a_j)||^2
              then choose B to minimize the SSD = summation_over_j(E_j^2)

           SSD_p is called the minimum residual variance for any basis of dimension p.

           SVD( Sample Covariance ) = SVD((1/(n-1)) * X^T*X)

           SVD(A^T*A).U and V are both == SVD(A).V
           SVD(A*A^T).U and V are both == SVD(A).U

             A = U * D * V^T  where all U and V^T are from SVD(A), and D is from SVD(A).s
             A^T A = V * D^2 * V^T
             SampleCov = (1/(n-1)) * A^T A
                       = (1/(n-1)) * V * D^2 * V^T

             we also have U * D = X * V from SVD

             Jepson:  B first vector is the first column of V in SVD(C_S)
                where C_S is the sample covariance and SVD(C_S).V == SVD(C_S).U ~ SVD(X).V

         SVD(C_S) = SVD((1/(n-1)) * X^T*X)
         SVD(X^T*X).U == SVD(X).V
            V gives the principal axes or principal directions of the dat
            X*V = U * D = principal components = projection of the data onto the principal axes
               and the coords of the newly transformed data are on row_0 for the first data point, etc.
        */

        // U is mxm
        // S is mxn
        // V is nxn

        double[][] u;
        double[] s;
        double[][] vT;
<span class="pc bpc" id="L182" title="1 of 2 branches missed.">        if (useSVD) {</span>
<span class="fc" id="L183">            SVD svd = SVD.factorize(new DenseMatrix(x));</span>
<span class="fc" id="L184">            s = svd.getS();</span>
<span class="fc" id="L185">            vT = MatrixUtil.convertToRowMajor(svd.getVt());</span>
<span class="fc" id="L186">            u = MatrixUtil.convertToRowMajor(svd.getU());</span>
<span class="fc" id="L187">        } else {</span>
<span class="nc" id="L188">            CURDecomposition.CUR cur = CURDecomposition.calculateDecomposition(x, nComponents);</span>
<span class="nc" id="L189">            MatrixUtil.SVDProducts svd = cur.getApproximateSVD();</span>
<span class="nc" id="L190">            s = svd.s;</span>
<span class="nc" id="L191">            vT = svd.vT;</span>
<span class="nc" id="L192">            u = svd.u;</span>
        }

        int i;
        int j;
<span class="fc" id="L197">        int rank = 0;</span>
<span class="fc bfc" id="L198" title="All 2 branches covered.">        for (i = 0; i &lt; s.length; ++i) {</span>
<span class="pc bpc" id="L199" title="1 of 2 branches missed.">            if (Math.abs(s[i]) &gt; eps) {</span>
<span class="fc" id="L200">                rank++;</span>
            }
        }

<span class="pc bpc" id="L204" title="1 of 2 branches missed.">        if (nComponents &gt; rank) {</span>
<span class="nc" id="L205">            nComponents = rank;</span>
        }

<span class="fc" id="L208">        double sumEvTrunc = 0;</span>
<span class="fc" id="L209">        double[] eig = new double[rank];</span>
<span class="fc" id="L210">        double sumEVTotal = 0;</span>
<span class="fc bfc" id="L211" title="All 2 branches covered.">        for (i = 0; i &lt; eig.length; ++i) {</span>
<span class="fc" id="L212">            eig[i] = (s[i] * s[i])/((double)n - 1.);</span>
<span class="fc" id="L213">            sumEVTotal += eig[i];</span>
<span class="pc bpc" id="L214" title="1 of 2 branches missed.">            if (i &lt; rank) {</span>
<span class="fc" id="L215">                sumEvTrunc = sumEVTotal;</span>
            }
        }
        //residual fractional eigenvalue is 1-(sum_{i=0 to nComponents-1}(s[i]) / sum_{i=0 to n-1}(s[i]))
<span class="fc" id="L219">        double residFracEigen = 1 - (sumEvTrunc/sumEVTotal);</span>

        // COLUMNS of v are the principal axes, a.k.a. principal directions

<span class="fc" id="L223">        double[][] pA = MatrixUtil.copySubMatrix(vT, 0, nComponents - 1, 0, vT[0].length - 1);</span>

        // X*V = U * diag(s) = principal components
<span class="fc" id="L226">        double[][] uP = MatrixUtil.zeros(u.length, nComponents);</span>
        // principal components for &quot;1 sigma&quot;.  if need 2 sigma, multiply pc by 2,...
<span class="fc" id="L228">        double[][] pC = MatrixUtil.zeros(u.length, nComponents);</span>
<span class="fc bfc" id="L229" title="All 2 branches covered.">        for (i = 0; i &lt; u.length; ++i) {</span>
<span class="fc bfc" id="L230" title="All 2 branches covered.">            for (j = 0; j &lt; nComponents; ++j) {</span>
                // row u_i * diag(s[j])
<span class="fc" id="L232">                pC[i][j] = u[i][j] * s[j];</span>
<span class="fc" id="L233">                uP[i][j] = u[i][j];</span>
            }
        }
        //checked, same as pC = U * diag(s)
        //double[][] xv = MatrixUtil.multiply(x, MatrixUtil.transpose(vT));
        //System.out.printf(&quot;xV=\n%s\n&quot;, FormatArray.toString(xv, &quot;%.4e&quot;));

<span class="fc" id="L240">        PCAStats stats = new PCAStats();</span>
<span class="fc" id="L241">        stats.nComponents = nComponents;</span>
<span class="fc" id="L242">        stats.principalAxes = pA;</span>
<span class="fc" id="L243">        stats.principalComponents = pC;</span>
<span class="fc" id="L244">        stats.eigenValues = eig;</span>
<span class="fc" id="L245">        stats.uP = uP;</span>
<span class="fc" id="L246">        stats._s = Arrays.copyOf(s, s.length);</span>

<span class="fc" id="L248">        double total = 0;</span>
<span class="fc bfc" id="L249" title="All 2 branches covered.">        for (j = 0; j &lt; eig.length; ++j) {</span>
<span class="fc" id="L250">            total += eig[j];</span>
        }
<span class="fc" id="L252">        double[] fracs = new double[rank];</span>
<span class="fc bfc" id="L253" title="All 2 branches covered.">        for (j = 0; j &lt; rank; ++j) {</span>
<span class="fc" id="L254">            fracs[j] = eig[j]/total;</span>
        }
<span class="fc" id="L256">        double[] c = Arrays.copyOf(fracs, fracs.length);</span>
<span class="fc bfc" id="L257" title="All 2 branches covered.">        for (j = 1; j &lt; c.length; ++j) {</span>
<span class="fc" id="L258">            c[j] += c[j-1];</span>
        }
<span class="fc" id="L260">        stats.cumulativeProportion = c;</span>

<span class="fc" id="L262">        double ssdP = 0;</span>
<span class="fc bfc" id="L263" title="All 2 branches covered.">        for (j = nComponents; j &lt; nDimensions; ++j) {</span>
<span class="fc" id="L264">            ssdP += eig[j];</span>
        }
<span class="fc" id="L266">        stats.ssdP = ssdP;</span>
<span class="fc" id="L267">        stats.fractionVariance = (total - ssdP)/total;</span>

<span class="fc" id="L269">        System.out.printf(&quot;nComponents==%d\n&quot;, nComponents);</span>
<span class="fc" id="L270">        System.out.printf(&quot;ssd_p=%.5e\n&quot;, stats.ssdP);</span>
<span class="fc" id="L271">        System.out.printf(&quot;fractionVariance=%.5e\n&quot;, stats.fractionVariance);</span>

<span class="fc" id="L273">        System.out.printf(&quot;eig = %s\n&quot;, FormatArray.toString(eig, &quot;%.4e&quot;));</span>
<span class="fc" id="L274">        System.out.printf(&quot;s = %s\n&quot;, FormatArray.toString(s, &quot;%.4e&quot;));</span>
<span class="fc" id="L275">        System.out.printf(&quot;s fractions of total = \n%s\n&quot;,</span>
<span class="fc" id="L276">                FormatArray.toString(fracs, &quot;%.4e&quot;));</span>
<span class="fc" id="L277">        System.out.printf(&quot;eigenvalue cumulativeProportion=\n%s\n&quot;,</span>
<span class="fc" id="L278">                FormatArray.toString(stats.cumulativeProportion, &quot;%.4e&quot;));</span>

        //System.out.printf(&quot;x=\n%s\n&quot;, FormatArray.toString(x, &quot;%.4e&quot;));
        //System.out.printf(&quot;v=\n%s\n&quot;, FormatArray.toString(MatrixUtil.transpose(vT), &quot;%.4e&quot;));
        //System.out.printf(&quot;u=\n%s\n&quot;, FormatArray.toString(u, &quot;%.4e&quot;));

<span class="fc" id="L284">        System.out.printf(&quot;principal axes=\n%s\n&quot;, FormatArray.toString(pA, &quot;%.4e&quot;));</span>
<span class="fc" id="L285">        System.out.printf(&quot;principal projections (=u_p*s)=\n%s\n&quot;, FormatArray.toString(pC, &quot;%.4e&quot;));</span>
<span class="fc" id="L286">        System.out.flush();</span>

        //checked, same as pC = U * diag(s)
        //double[][] xv = MatrixUtil.multiply(x, MatrixUtil.transpose(vT));
        //System.out.printf(&quot;xV=\n%s\n&quot;, FormatArray.toString(xv, &quot;%.4e&quot;));

<span class="fc" id="L292">        return stats;</span>
    }

    /**
     * reconstruct an image from x, given principal directions.
     * from http://www.cs.toronto.edu/~jepson/csc420/
     * combined with the book by Strang &quot;Introduction to Linear Algebra&quot; 
     * and the book by Leskovec, Rajaraman, and Ullman &quot;Mining of Massive Datasets&quot;
     * &lt;pre&gt;
     *     ⃗r(⃗a_0) = m⃗_s + U_p * ⃗a 
     *         where U_p is the principalDirections matrix,
     *         where m_s is the sample x mean,
     *         where a_0 is
     *            a_0 = arg min_{a} ||⃗x − (m⃗_s + U _p * a)||^2
     * &lt;/pre&gt;
     *
     * @param m the means of each column of x (used in the zero-correction of x).  the length is k
     *          where k is the number of columns in x before
     *          dimension reduction.
     * @param stats the statistics of the principal axes derived from
     * SVD of the covariance of the training data.
     * @return the reconstruction of x.  size is [n x k]
     */
    public static double[][] reconstruct(double[] m, PCAStats stats) {

<span class="fc" id="L317">        int p = stats.nComponents;</span>
<span class="fc" id="L318">        int k = stats._s.length;</span>
<span class="fc" id="L319">        int n = stats.principalComponents.length;</span>

<span class="pc bpc" id="L321" title="1 of 2 branches missed.">        if (n != stats.uP.length) {</span>
<span class="nc" id="L322">            throw new IllegalArgumentException(&quot;n must be stats.principalComponents.length which must be equal to &quot; +</span>
                    &quot;stats.principalComponents.length&quot;);
        }
<span class="pc bpc" id="L325" title="1 of 2 branches missed.">        if (k != m.length) {</span>
<span class="nc" id="L326">            throw new IllegalArgumentException(&quot;m.length must equal k == stats.principalAxes.length&quot;);</span>
        }

        //U_p is stats.uP

        //find the 'a' which minimizes ||⃗x − (m⃗_s + p * a)||^2
        //then the reconstruction is r(⃗a ) = m⃗ + U ⃗a

        // U_p is [n X p]
        // x_j is [n X 1]
        // a_j is [p X 1]
        // A is [p X k]

        // reconstruction of x0 is
        //   U_p * a_0
        //   [n X p] * [p X 1] = [n X 1];

        int i;
        int j;

<span class="fc" id="L346">        double[][] re = MatrixUtil.multiply(stats.principalComponents,</span>
                stats.principalAxes);
        //reconstructions = stats.principalComponents dot stats.pA + column means
        //    which is u_P * s dot pA
        //[n X p] * [p X k] = [n X k]
<span class="fc bfc" id="L351" title="All 2 branches covered.">        for (i = 0; i &lt; re.length; ++i) {</span>
<span class="fc bfc" id="L352" title="All 2 branches covered.">            for (j = 0; j &lt; re[i].length; ++j) {</span>
<span class="fc" id="L353">                re[i][j] += m[j];</span>
            }
        }
<span class="fc" id="L356">        return re;</span>
    }
    
    public static class PCAStats {

        /** the number of components requested from the calculation, unless larger than the matrix
         * rank in which case the value will be the rank.
         */
        int nComponents;

        /**
         * principal axes a.k.a. principal directions of the data.
         * These are the first nComponents columns of the SVD(X).V matrix.
         * When considering X as sample covariance, Note that SVD(A^T*A).U and V are both == SVD(A).V.
         * the size is [k X k] where k is x[0].length.
         */
        double[][] principalAxes;

        /**
         * principal components are the data projected onto the principal axes.
         * principal components = X*V = U * D where X = SVD(X).U * diag(SVD(X).s) * SVD(X).vT
         * The size is [n X p] where n is x.length and p is the number of components.
         */
        double[][] principalComponents;
        
        /**
         * the fraction of the total variance Q_p (where p is the dimension number, that is, nComponents):
         * &lt;pre&gt;
         *    Q_p = (SSD_0 − SSD_p)/SSD_0
         * 
         *    where SSD_p = summation_{j=p+1 to n}(s_j)
         *       where s_j is from the diagonal matrix S of the SVD
         *          of the covariance of x.
         * &lt;/pre&gt;
         */
        double fractionVariance;
        
        /**
         * the cumulative addition of 
         *     (sum_of_eigenvalues - eigenvalue_i)/sum_of_eigenvalues
         *     after truncation.  that is, the sum of eigenvalues is the sum of the truncated number of eigenvalues,
         *      not the entire number from the original decomposition.
         */
        double[] cumulativeProportion;
        
        /**
         * Minimum residual variance
         */
        double ssdP;
        
        /**
         * the first nComponents number of eigenValues
         */
        double[] eigenValues;

        /**
         * the singular values of SVD(A)
         */
        double[] _s;

        /**
         * the first p eigenvectors of the left eigenvector matrix
         * (which is p columns of U from A = SVD(A).U * SVD(A).D * SVD(A).V^T
         */
        double[][] uP;

    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.8.202204050719</span></div></body></html>