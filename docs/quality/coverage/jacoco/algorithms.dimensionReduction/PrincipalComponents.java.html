<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>PrincipalComponents.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Jacoco Report</a> &gt; <a href="index.source.html" class="el_package">algorithms.dimensionReduction</a> &gt; <span class="el_source">PrincipalComponents.java</span></div><h1>PrincipalComponents.java</h1><pre class="source lang-java linenums">package algorithms.dimensionReduction;

import algorithms.correlation.BruteForce;
import algorithms.matrix.MatrixUtil;
import java.util.Arrays;
import no.uib.cipr.matrix.DenseMatrix;
import no.uib.cipr.matrix.Matrices;
import no.uib.cipr.matrix.NotConvergedException;
import no.uib.cipr.matrix.SVD;

/**
 * find the principal components, that is, the minimal residual variance bases
 * of vectors of samples following G. Strang's SVD in machine learning in
 * http://www.cs.toronto.edu/~jepson/csc420/
 * 
 * from wikipedia:
 * The principal components of a collection of points in a real coordinate space 
 * are a sequence of p unit vectors, where the i-th vector is the direction of 
 * a line that best fits the data while being orthogonal to the first i-1 vectors. 
 * Here, a best-fitting line is defined as one that minimizes the average 
 * squared distance from the points to the line. These directions constitute an 
 * orthonormal basis in which different individual dimensions of the data are 
 * linearly uncorrelated. Principal component analysis (PCA) is the process of 
 * computing the principal components and using them to perform a change of basis 
 * on the data, sometimes using only the first few principal components and 
 * ignoring the rest.
 * 
 * from wikipedia:
 * 
 * PCA is closely related to Fisher's Discriminant Anaylsisi a.k.a. 
 * Linear Discriminant analysis and factor analysis in that they both look for 
 * linear combinations of variables which best explain the data.[4] 
 * LDA explicitly attempts to model the difference between the classes of data. 
 * PCA, in contrast, does not take into account any difference in class, 
 * and factor analysis builds the feature combinations based on differences 
 * rather than similarities.
 * NOTE:  Fisher's original article[1] actually describes a slightly different 
 * discriminant, which does not make some of the assumptions of LDA such as 
 * normally distributed classes or equal class covariances.
 * 
 * LDA is closely related to analysis of variance (ANOVA) and regression analysis, 
 * which also attempt to express one dependent variable as a linear combination 
 * of other features or measurements.[1][2] However, ANOVA uses categorical 
 * independent variables and a continuous dependent variable, whereas 
 * discriminant analysis has continuous independent variables and a categorical 
 * dependent variable (i.e. the class label).[3] Logistic regression and 
 * probit regression are more similar to LDA than ANOVA is, as they also 
 * explain a categorical variable by the values of continuous independent 
 * variables. These other methods are preferable in applications where it is 
 * not reasonable to assume that the independent variables are normally 
 * distributed, which is a fundamental assumption of the LDA method.
 * 
 * @author nichole
 */
<span class="pc bpc" id="L55" title="1 of 2 branches missed.">public class PrincipalComponents {</span>
    
    /**
     * calculate the principal components of the unit standardized data x
     * using SVD.
     * NOTE: should standardize the data before using this method,
     * &lt;pre&gt;
     *      double[] mean = new double[data[0].length];
            double[] stDev = new double[data[0].length];
     * e.g. double[][] x = Standardization.standardUnitNormalization(data, mean, stDev);
     * &lt;/pre&gt;
     * 
     * from http://www.cs.toronto.edu/~jepson/csc420/
     * combined with the book by Strang &quot;Introduction to Linear Algebra&quot; 
     * and the book by Leskovec, Rajaraman, and Ullman &quot;Mining of Massive Datasets&quot;.
     * also useful:
     * https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca
     * and https://online.stat.psu.edu/stat505/book/export/html/670
     * 
     * NOTE: SVD has the drawbacks that a singular vector specifies a linear
       combination of all input columns or rows, and there is a
       a Lack of sparsity in the U, V, and s matrices.
       See CUR decomposition method in contrast.
       http://www.mmds.org/mmds/v2.1/ch11-dimred.pdf
     * @param x is a 2-dimensional array of k vectors of length n in format
     *    double[n][k].  n is the number of samples, and k is the number of
     *    variables, a.k.a. dimensions.
     * @param nComponents the number of principal components to return.
     * @return a few statistics of the SVD of the covariance of A, up to the
     * nComponents dimension.  Note that if the rank of the SVD(cov(A)) is
     * less than nComponents, then only that amount is returned.
     */
    public static PCAStats calcPrincipalComponents(double[][] x, int nComponents) throws NotConvergedException {
                
<span class="fc" id="L89">        int n = x.length;</span>
<span class="fc" id="L90">        int nDimensions = x[0].length;</span>
        
<span class="pc bpc" id="L92" title="1 of 2 branches missed.">        if (nComponents &gt; nDimensions) {</span>
<span class="nc" id="L93">            throw new IllegalArgumentException(&quot;x has &quot; + nDimensions + &quot; but &quot; </span>
                + &quot; nComponents principal components were requested.&quot;);
        }
        
<span class="fc" id="L97">        double eps = 1.e-15;</span>
           
        /*
        NOTE that the Strang approach does not assume the data has been
        standardized to mean 0 and unit variance.
        
        minimal residual variance basis:
           basis direction b.
           samples are vectors x_j where j = 0 to k
           sample mean m_s = (1/k) summation_over_j(x_j)

           looking for a p-dimensional basis that minimizes:
              SSD_p = min_{B}( summation_over_j( min_{a_j}(||x_j - (m_s + B*a_j)||^2) ) )
                 where B = (b1, . . . , bp) is the n × p matrix formed from the selected basis.

              choose the coefficients ⃗aj which minimize the least squares error
                 E_j^2 = ||x_j - (m_s + B*a_j)||^2
              then choose B to minimize the SSD = summation_over_j(E_j^2)

           SSD_p is called the minimum residual variance for any basis of dimension p.

           [U, S, V] = SVD( Cov )
           the 1st principal direction is the 1st column of U.
        */
    
<span class="fc" id="L122">        double[][] cov = BruteForce.covariance(x);</span>
<span class="pc bpc" id="L123" title="3 of 4 branches missed.">        assert(nDimensions == cov.length);</span>
<span class="pc bpc" id="L124" title="3 of 4 branches missed.">        assert(nDimensions == cov[0].length);</span>
                        
        //NOTE: we know that cov is symmetric positive definite, so there are
        //   many operations special to it one could explore for diagonalization
        
<span class="fc" id="L129">        SVD svd = SVD.factorize(new DenseMatrix(cov));</span>
        // U is mxm
        // S is mxn
        // V is nxn
        
<span class="fc" id="L134">        double[] s = svd.getS();</span>
        
<span class="fc" id="L136">        int rank = 0;</span>
<span class="fc bfc" id="L137" title="All 2 branches covered.">        for (int i = 0; i &lt; s.length; ++i) {</span>
<span class="pc bpc" id="L138" title="1 of 2 branches missed.">            if (Math.abs(s[i]) &gt; eps) {</span>
<span class="fc" id="L139">                rank++;</span>
            }
        }
        
<span class="pc bpc" id="L143" title="1 of 2 branches missed.">        if (nComponents &gt; rank) {</span>
<span class="nc" id="L144">            nComponents = rank;</span>
        }
        
        //eigenvalue_i = lambda_i = (s_i^2)/(n-1)
<span class="fc" id="L148">        double[] eV = Arrays.copyOf(s, rank);</span>
<span class="fc bfc" id="L149" title="All 2 branches covered.">        for (int i = 0; i &lt; eV.length; ++i) {</span>
<span class="fc" id="L150">            eV[i] *= eV[i];</span>
<span class="fc" id="L151">            eV[i] /= ((double)n - 1.);</span>
        }
        
        // COLUMNS of u are the principal axes, a.k.a. principal directions
        // size is nDimensions x nDimensions
<span class="fc" id="L156">        DenseMatrix u = svd.getU();</span>
<span class="pc bpc" id="L157" title="3 of 4 branches missed.">        assert(nDimensions == u.numRows());</span>
<span class="pc bpc" id="L158" title="3 of 4 branches missed.">        assert(nDimensions == u.numColumns());</span>
                     
        // extract the nComponents columns of U as the principal axes, a.k.a. 
        //  principal directions.  array format: [nDimensions][nComponents]
<span class="fc" id="L162">        double[][] pa = new double[u.numRows()][nComponents];                                </span>
<span class="fc bfc" id="L163" title="All 2 branches covered.">        for (int row = 0; row &lt; u.numRows(); ++row) {</span>
<span class="fc" id="L164">            pa[row] = new double[nComponents];</span>
<span class="fc bfc" id="L165" title="All 2 branches covered.">            for (int col = 0; col &lt; nComponents; ++col) {</span>
<span class="fc" id="L166">                pa[row][col] = u.get(row, col);</span>
            }
        }
        
        // NOTE: the principal scores is a vector Y_1 of length n:
        //     first principal score Y_1 = 
        //         sum of (first principal direction for each variable dot x[*][variable] )
        //  The magnitudes of the principal direction coefficients give the 
        //  contributions of each variable to that component. 
        //  Because the data have been standardized, they do not depend on the 
        //  variances of the corresponding variables.      
        
        // principal components are the projections of the principal axes on U
        //   the &quot;1 sigma&quot; lengths are:
<span class="fc" id="L180">        double[][] projections = new double[pa.length][pa[0].length];</span>
<span class="fc bfc" id="L181" title="All 2 branches covered.">        for (int row = 0; row &lt; pa.length; ++row) {</span>
<span class="fc" id="L182">            projections[row] = Arrays.copyOf(pa[row], pa[row].length);</span>
<span class="fc bfc" id="L183" title="All 2 branches covered.">            for (int col = 0; col &lt; pa[row].length; ++col) {</span>
                // note, if wanted &quot;3 sigma&quot; instead, use factor 3*s[col] here:
<span class="fc" id="L185">                projections[row][col] *= s[col];</span>
            }
        }
        
        // extract the first nComponents of rows of V^T:
<span class="fc" id="L190">        double[][] v = MatrixUtil.convertToRowMajor(svd.getVt());</span>
<span class="fc" id="L191">        v = MatrixUtil.copySubMatrix(v, 0, nComponents-1, 0, v[0].length-1);</span>
        
<span class="fc" id="L193">        PCAStats stats = new PCAStats();</span>
<span class="fc" id="L194">        stats.nComponents = nComponents;</span>
<span class="fc" id="L195">        stats.principalDirections = pa;</span>
<span class="fc" id="L196">        stats.eigenvalues = eV;</span>
<span class="fc" id="L197">        stats.vTP = v;</span>
        
<span class="fc" id="L199">        stats.s = new double[nComponents];</span>
<span class="fc bfc" id="L200" title="All 2 branches covered.">        for (int i = 0; i &lt; nComponents; ++i) {</span>
<span class="fc" id="L201">            stats.s[i] = s[i];</span>
        }
                
<span class="fc" id="L204">        double total = 0;</span>
<span class="fc bfc" id="L205" title="All 2 branches covered.">        for (int j = 0; j &lt; rank; ++j) {</span>
<span class="fc" id="L206">            total += s[j];</span>
        }
<span class="fc" id="L208">        double[] fracs = new double[rank];</span>
<span class="fc bfc" id="L209" title="All 2 branches covered.">        for (int j = 0; j &lt; rank; ++j) {</span>
<span class="fc" id="L210">            fracs[j] = s[j]/total;</span>
        }
<span class="fc" id="L212">        double[] c = Arrays.copyOf(fracs, fracs.length);</span>
<span class="fc bfc" id="L213" title="All 2 branches covered.">        for (int j = 1; j &lt; c.length; ++j) {</span>
<span class="fc" id="L214">            c[j] += c[j-1];</span>
        }
<span class="fc" id="L216">        stats.cumulativeProportion = c;</span>
        
<span class="fc" id="L218">        double sum = 0;</span>
        int p;
<span class="fc bfc" id="L220" title="All 2 branches covered.">        for (int j = (nComponents+1); j &lt;= nDimensions; ++j) {</span>
<span class="fc" id="L221">            p = j - 1;</span>
<span class="fc" id="L222">            sum += s[p];</span>
        }
<span class="fc" id="L224">        stats.ssdP = sum;</span>
<span class="fc" id="L225">        stats.fractionVariance = (total - sum)/total;</span>
        
<span class="fc" id="L227">        System.out.println(&quot;singular values of SVD(cov) = sqrts of eigenvalues of cov = &quot;);</span>
<span class="fc bfc" id="L228" title="All 2 branches covered.">        for (int i = 0; i &lt; rank; ++i) {</span>
<span class="fc" id="L229">            System.out.printf(&quot;%11.3e  &quot;, s[i]);</span>
        }
<span class="fc" id="L231">        System.out.println();</span>
<span class="fc" id="L232">        System.out.println(&quot;eigenvalues of cov = &quot;);</span>
<span class="fc bfc" id="L233" title="All 2 branches covered.">        for (int i = 0; i &lt; eV.length; ++i) {</span>
<span class="fc" id="L234">            System.out.printf(&quot;%11.3e  &quot;, eV[i]);</span>
        }
<span class="fc" id="L236">        System.out.println();</span>
<span class="fc" id="L237">        System.out.println(&quot;U of SVD(cov) = &quot;);</span>
<span class="fc bfc" id="L238" title="All 2 branches covered.">        for (int i = 0; i &lt; u.numRows(); ++i) {</span>
<span class="fc bfc" id="L239" title="All 2 branches covered.">            for (int j = 0; j &lt; u.numColumns(); ++j) {</span>
<span class="fc" id="L240">                System.out.printf(&quot;%12.5e  &quot;, u.get(i, j));</span>
            }
<span class="fc" id="L242">            System.out.printf(&quot;\n&quot;);</span>
        }
<span class="fc" id="L244">        System.out.println(&quot;V_p of SVD(cov) = &quot;);</span>
<span class="fc bfc" id="L245" title="All 2 branches covered.">        for (int i = 0; i &lt; v.length; ++i) {</span>
<span class="fc bfc" id="L246" title="All 2 branches covered.">            for (int j = 0; j &lt; v[i].length; ++j) {</span>
<span class="fc" id="L247">                System.out.printf(&quot;%12.5e  &quot;, v[i][j]);</span>
            }
<span class="fc" id="L249">            System.out.printf(&quot;\n&quot;);</span>
        }
        
<span class="fc" id="L252">        System.out.println(&quot;eigenvalue fractions of total = &quot;);</span>
<span class="fc bfc" id="L253" title="All 2 branches covered.">        for (int i = 0; i &lt; fracs.length; ++i) {</span>
<span class="fc" id="L254">            System.out.printf(&quot;%11.3e  &quot;, fracs[i]);</span>
        }
<span class="fc" id="L256">        System.out.println();</span>
<span class="fc" id="L257">        System.out.println(&quot;eigenvalue cumulativeProportion= &quot;);</span>
<span class="fc bfc" id="L258" title="All 2 branches covered.">        for (int i = 0; i &lt; stats.cumulativeProportion.length; ++i) {</span>
<span class="fc" id="L259">            System.out.printf(&quot;%11.3e  &quot;, stats.cumulativeProportion[i]);</span>
        }
<span class="fc" id="L261">        System.out.println();</span>
        
<span class="fc" id="L263">        System.out.println(&quot;principal directions= &quot;);</span>
<span class="fc bfc" id="L264" title="All 2 branches covered.">        for (int i = 0; i &lt; stats.principalDirections.length; ++i) {</span>
<span class="fc bfc" id="L265" title="All 2 branches covered.">            for (int j = 0; j &lt; stats.principalDirections[i].length; ++j) {</span>
<span class="fc" id="L266">                System.out.printf(&quot;%12.5e  &quot;, stats.principalDirections[i][j]);</span>
            }
<span class="fc" id="L268">            System.out.printf(&quot;\n&quot;);</span>
        }
<span class="fc" id="L270">        System.out.println(&quot;principal projections (=u_p*s)=&quot;);</span>
<span class="fc bfc" id="L271" title="All 2 branches covered.">        for (int i = 0; i &lt; projections.length; ++i) {</span>
<span class="fc bfc" id="L272" title="All 2 branches covered.">            for (int j = 0; j &lt; projections[i].length; ++j) {</span>
<span class="fc" id="L273">                System.out.printf(&quot;%11.3e  &quot;, projections[i][j]);</span>
            }
<span class="fc" id="L275">            System.out.printf(&quot;\n&quot;);</span>
        }
        
<span class="fc" id="L278">        System.out.flush();</span>
        
<span class="fc" id="L280">        return stats;</span>
    }
    
    /**
     * reconstruct an image from x, given principal directions.
     * from http://www.cs.toronto.edu/~jepson/csc420/
     * combined with the book by Strang &quot;Introduction to Linear Algebra&quot; 
     * and the book by Leskovec, Rajaraman, and Ullman &quot;Mining of Massive Datasets&quot;
     * &lt;pre&gt;
     *     ⃗r(⃗a_0) = m⃗_s + U_p * ⃗a 
     *         where U_p is the principalDirections matrix,
     *         where m_s is the sample x mean,
     *         where a_0 is
     *            a_0 = arg min_{a} ||⃗x − (m⃗_s + U _p * a)||^2
     * &lt;/pre&gt;
     * @param x a 2-dimensional array of k vectors of length n in format
     *    double[n][k] which is double[nSamples][nDimensions] for which to apply
     *    the principal axes transformation on.
     * @param stats the statistics of the principal axes derived from
     * SVD of the covariance of the training data.
     * @return 
     */
    public static double[][] reconstruct(double[][] x, PCAStats stats) {
        
        /* find the 'a' which minimizes ||⃗x − (m⃗_s + p * a)||^2
        ⃗
        then the reconstruction is r(⃗a ) = m⃗ + U ⃗a 
        */
       
<span class="fc" id="L309">        double[][] b = MatrixUtil.multiply(x, stats.principalDirections);</span>
<span class="fc" id="L310">        b = MatrixUtil.multiply(b, stats.vTP);</span>
        
<span class="fc" id="L312">        return b;</span>
    }
    
    public static class PCAStats {
        /** the number of components requested from the calculation, that is,
         the first p principal components from the U matrix of the SVD 
         decomposition of the covariance of x.
         * NOTE that x given to the algorithm, is a sample of vectors 
         *    {⃗x_j}_{j=1 to k}, with each ⃗x_j ∈ R^n (each x_j is a vector of 
         *    n real numbers) that form the matrix X ∈ R^(nxk).
. 
         */
        int nComponents;
        
        /**
         * the first nComponents principal directions of x. 
         * this is obtained from the first p columns of the U matrix of SVD(cov(x)).
         * the U_p columns have been transposed into rows here:
         * principalDirections[0] holds 1st principal direction,
         * principalDirections[1] holds 2nd principal direction, etc.
         * &lt;pre&gt;
         *    x * principalDirections * v^T_p gives the principal axes which
         *       minimize the variance.
         * &lt;/pre&gt;
         */
        double[][] principalDirections;
        
        /**
         * the first p rows of the V^T matrix of SVD(cov(x)).
         * &lt;pre&gt;
         *    x * principalDirections * v^T_p gives the principal axes which
         *       minimize the variance.
         * &lt;/pre&gt;
         */
        double[][] vTP;
        
        /**
         * the fraction of the total variance Q_p (where p is the dimension
         * number, that is, nComponents):
         * &lt;pre&gt;
         *    Q_p = (SSD_0 − SSD_p)/SSD_0
         * 
         *    where SSD_p = summation_{j=p+1 to n}(s_j)
         *       where s_j is from the diagonal matrix S of the SVD
         *          of the covariance of x.
         * &lt;/pre&gt;
         */
        double fractionVariance;
        
        /**
         * the cumulative addition of 
         *     (sum_of_eigenvalues - eigenvalue_i)/sum_of_eigeneigenvalues
         */
        double[] cumulativeProportion;
        
        /**
         * Minimum residual variance
         */
        double ssdP;
        
        /**
         * the first nComponents singular values from the diagonal matrix of
         * the SVD of covariance of x;
         */
        double[] s;
        double[] eigenvalues;
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.9.201702052155</span></div></body></html>