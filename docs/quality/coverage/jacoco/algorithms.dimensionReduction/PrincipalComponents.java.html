<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>PrincipalComponents.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Jacoco Report</a> &gt; <a href="index.source.html" class="el_package">algorithms.dimensionReduction</a> &gt; <span class="el_source">PrincipalComponents.java</span></div><h1>PrincipalComponents.java</h1><pre class="source lang-java linenums">package algorithms.dimensionReduction;

import algorithms.matrix.MatrixUtil;
import java.util.Arrays;

import algorithms.util.FormatArray;
import no.uib.cipr.matrix.DenseMatrix;
import no.uib.cipr.matrix.NotConvergedException;
import no.uib.cipr.matrix.SVD;

/**
 * find the principal components, that is, the minimal residual variance bases
 * of vectors of samples following G. Strang's SVD in machine learning in
 * http://www.cs.toronto.edu/~jepson/csc420/
 * 
 * from wikipedia:
 * The principal components of a collection of points in a real coordinate space 
 * are a sequence of p unit vectors, where the i-th vector is the directionCCW of
 * a line that best fits the data while being orthogonal to the first i-1 vectors. 
 * Here, a best-fitting line is defined as one that minimizes the average 
 * squared distance from the points to the line. These directions constitute an 
 * orthonormal basis in which different individual dimensions of the data are 
 * linearly uncorrelated. Principal component analysis (PCA) is the process of 
 * computing the principal components and using them to perform a change of basis 
 * on the data, sometimes using only the first few principal components and 
 * ignoring the rest.
 * 
 * from wikipedia:
 * 
 * PCA is closely related to Fisher's Discriminant Anaylsisi a.k.a. 
 * Linear Discriminant analysis and factor analysis in that they both look for 
 * linear combinations of variables which best explain the data.[4] 
 * LDA explicitly attempts to model the difference between the classes of data. 
 * PCA, in contrast, does not take into account any difference in class, 
 * and factor analysis builds the feature combinations based on differences 
 * rather than similarities.
 * NOTE:  Fisher's original article[1] actually describes a slightly different 
 * discriminant, which does not make some of the assumptions of LDA such as 
 * normally distributed classes or equal class covariances.
 * 
 * LDA is closely related to analysis of variance (ANOVA) and regression analysis, 
 * which also attempt to express one dependent variable as a linear combination 
 * of other features or measurements.[1][2] However, ANOVA uses categorical 
 * independent variables and a continuous dependent variable, whereas 
 * discriminant analysis has continuous independent variables and a categorical 
 * dependent variable (i.e. the class label).[3] Logistic regression and 
 * probit regression are more similar to LDA than ANOVA is, as they also 
 * explain a categorical variable by the values of continuous independent 
 * variables. These other methods are preferable in applications where it is 
 * not reasonable to assume that the independent variables are normally 
 * distributed, which is a fundamental assumption of the LDA method.
 * 
 * @author nichole
 */
<span class="nc" id="L55">public class PrincipalComponents {</span>

    /**
     * calculate the principal components of the unit standardized data x
     * using Singular Value Decomposition.
     * NOTE: the data need to be zero centered first.
     *
     &lt;pre&gt;
     the method follows:
     http://www.cs.toronto.edu/~jepson/csc420/
     combined with the book by Strang &quot;Introduction to Linear Algebra&quot;
     also useful:
     https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca
     and https://online.stat.psu.edu/stat505/book/export/html/670 for testing results,
     &lt;/pre&gt;
     &lt;pre&gt;
     NOTE:
     variance-covariance(standardized data) == correlation(unstandardized data) == correlation(zero mean centered data).
     therefore, pca using the standardized data == pca using the correlation matrix.
     Also, the eigen of cov(standardized) == eigen of cor(unstandardized).
     &lt;/pre&gt;
     @param x is a 2-dimensional array of k vectors of length n in format
     *    double[n][k].  n is the number of samples, and k is the number of
     *    variables, a.k.a. dimensions.
     *    x should be zero-centered (mean=0) OR standardized to unit normalization (which is mean=0, stdev=1).
     *    If the variance and scale of the variables are different, then unit standard normalization should be used.
     Note that using &quot;zero mean centered&quot; x is equivalent to pca on the covariance matrix,
      *        while using &quot;unit standard norm: x is equivalent to pca on the correlation matrix.
     @param nComponents the number of principal components to return.
     @return the principal axes, the principal components, and
     a few statistics of the CUR decomposition of A, up to the
     nComponents dimension.  Note that if the rank of the A is
     less than nComponents, then only the number of components as rank is returned.
     * @throws no.uib.cipr.matrix.NotConvergedException
     */
    public static PCAStats calcPrincipalComponents(double[][] x, int nComponents) throws NotConvergedException {
<span class="fc" id="L91">        return calcPrincipalComponents(x, nComponents, false);</span>
    }

    /**
     * calculate the principal components of the unit standardized data x
     * using Singular Value Decomposition or CUR Decomposition.
     * NOTE: the data need to be zero centered first.
     *
     &lt;pre&gt;
     the method follows:
     http://www.cs.toronto.edu/~jepson/csc420/
     combined with the book by Strang &quot;Introduction to Linear Algebra&quot;
     and the book by Leskovec, Rajaraman, and Ullman &quot;Mining of Massive Datasets&quot;.
     also useful:
     https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca
     and https://online.stat.psu.edu/stat505/book/export/html/670
     &lt;/pre&gt;
     &lt;pre&gt;
     NOTE:
     variance-covariance(standardized data) == correlation(unstandardized data) == correlation(zero mean centered data).
     therefore, pca using the standardized data == pca using the correlation matrix.
     Also, the eigen of cov(standardized) == eigen of cor(unstandardized).
     &lt;/pre&gt;
     @param x is a 2-dimensional array of k vectors of length n in format
     *    double[n][k].  n is the number of samples, and k is the number of
     *    variables, a.k.a. dimensions.
     *    x should be zero-centered (mean=0) OR standardized to unit normalization (which is mean=0, stdev=1).
     *    If the variance and scale of the variables are different, then unit standard normalization should be used.
     *    Note that using &quot;zero mean centered&quot; x is equivalent to pca on the covariance matrix,
     *        while using &quot;unit standard norm: x is equivalent to pca on the correlation matrix.
     @param useEVProp if true, uses the cumulative fraction of eigenvalues in combination with the
     proportion prop to determine the number of principal components to calculate, else if false,
     uses the cumulative fraction of singular values in combination with the proportaion prop to
     determine the number of principal components.  Note that the closest cumulative fraction to prop is used
     rather than meeting and possibly exceeding prop.
     e.g.  prop of 80% to 90% w/ useEVProp = true.
     @param prop
     @return the principal axes, the principal components, and
     a few statistics of the CUR decomposition of A, up to the
     nComponents dimension.  Note that if the rank of the A is
     less than nComponents, then only the number of components as rank is returned.
     * @throws no.uib.cipr.matrix.NotConvergedException
     */
    public static PCAStats calcPrincipalComponents(double[][] x, boolean useEVProp,
                                                   double prop) throws NotConvergedException {

        /*
        minimal residual variance basis:
           basis directionCCW b.
           samples are vectors x_j where j = 0 to k
           sample mean m_s = (1/k) summation_over_j(x_j)

           looking for a p-dimensional basis that minimizes:
              SSD_p = min_{B}( summation_over_j( min_{a_j}(||x_j - (m_s + B*a_j)||^2) ) )
                 where B = (b1, . . . , bp) is the n × p matrix formed from the selected basis.

              choose the coefficients ⃗aj which minimize the least squares error
                 E_j^2 = ||x_j - (m_s + B*a_j)||^2
              then choose B to minimize the SSD = summation_over_j(E_j^2)

           SSD_p is called the minimum residual variance for any basis of dimension p.

           SVD( Sample Covariance ) = SVD((1/(n-1)) * X^T*X)

           SVD(A^T*A).U and V are both == SVD(A).V
           SVD(A*A^T).U and V are both == SVD(A).U

             A = U * D * V^T  where all U and V^T are from SVD(A), and D is from SVD(A).s
             A^T A = V * D^2 * V^T
             SampleCov = (1/(n-1)) * A^T A
                       = (1/(n-1)) * V * D^2 * V^T

             we also have U * D = X * V from SVD

             Jepson:  B first vector is the first column of V in SVD(C_S)
                where C_S is the sample covariance and SVD(C_S).V == SVD(C_S).U ~ SVD(X).V

         SVD(C_S) = SVD((1/(n-1)) * X^T*X)
         SVD(X^T*X).U == SVD(X).V
            V gives the principal axes or principal directions of the dat
            X*V = U * D = principal components = projection of the data onto the principal axes
               and the coords of the newly transformed data are on row_0 for the first data point, etc.
        */
<span class="fc" id="L174">        int n = x.length;</span>
<span class="fc" id="L175">        int nDimensions = x[0].length;</span>

<span class="fc" id="L177">        double eps = 1.e-15;</span>

        // U is mxm
        // S is mxn
        // V is nxn

        double[][] u;
        double[] s;
        double[][] vT;
<span class="fc" id="L186">        SVD svd = SVD.factorize(new DenseMatrix(x));</span>
<span class="fc" id="L187">        s = svd.getS();</span>
<span class="fc" id="L188">        vT = MatrixUtil.convertToRowMajor(svd.getVt());</span>
<span class="fc" id="L189">        u = MatrixUtil.convertToRowMajor(svd.getU());</span>

        int i;
        int j;
<span class="fc" id="L193">        int rank = 0;</span>
<span class="fc bfc" id="L194" title="All 2 branches covered.">        for (i = 0; i &lt; s.length; ++i) {</span>
<span class="pc bpc" id="L195" title="1 of 2 branches missed.">            if (Math.abs(s[i]) &gt; eps) {</span>
<span class="fc" id="L196">                rank++;</span>
            }
        }

<span class="fc" id="L200">        double[] eig = new double[rank];</span>
<span class="fc" id="L201">        double sumEVTotal = 0;</span>
<span class="fc" id="L202">        double sumSTotal = 0;</span>
<span class="fc bfc" id="L203" title="All 2 branches covered.">        for (i = 0; i &lt; eig.length; ++i) {</span>
<span class="fc" id="L204">            eig[i] = (s[i] * s[i])/((double)n - 1.);</span>
<span class="fc" id="L205">            sumEVTotal += eig[i];</span>
<span class="fc" id="L206">            sumSTotal += s[i];</span>
        }

<span class="fc" id="L209">        double[] fracs = new double[rank];</span>
<span class="pc bpc" id="L210" title="1 of 2 branches missed.">        if (useEVProp) {</span>
<span class="fc bfc" id="L211" title="All 2 branches covered.">            for (j = 0; j &lt; fracs.length; ++j) {</span>
<span class="fc" id="L212">                fracs[j] = eig[j]/sumEVTotal;</span>
            }
        } else {
<span class="nc bnc" id="L215" title="All 2 branches missed.">            for (j = 0; j &lt; fracs.length; ++j) {</span>
<span class="nc" id="L216">                fracs[j] = s[j]/sumSTotal;</span>
            }
        }

        // find smallest diff between cumulative fraction and prop

<span class="fc" id="L222">        double[] c = Arrays.copyOf(fracs, fracs.length);</span>
<span class="fc" id="L223">        int idxM = 0;</span>
<span class="fc" id="L224">        double diff = Math.abs(c[0] - prop);</span>
<span class="fc" id="L225">        double min = diff;</span>
<span class="fc bfc" id="L226" title="All 2 branches covered.">        for (j = 1; j &lt; c.length; ++j) {</span>
<span class="fc" id="L227">            c[j] += c[j-1];</span>
<span class="fc" id="L228">            diff = Math.abs(c[j] - prop);</span>
<span class="fc bfc" id="L229" title="All 2 branches covered.">            if (diff &lt; min) {</span>
<span class="fc" id="L230">                idxM = j;</span>
<span class="fc" id="L231">                min = diff;</span>
            }
        }
<span class="fc" id="L234">        int nComponents = idxM + 1;</span>

<span class="fc" id="L236">        System.out.printf(&quot;nComponents=%d\n&quot;, nComponents);</span>
<span class="fc" id="L237">        System.out.printf(&quot;eig = %s\n&quot;, FormatArray.toString(eig, &quot;%.4e&quot;));</span>
<span class="fc" id="L238">        System.out.printf(&quot;s = %s\n&quot;, FormatArray.toString(s, &quot;%.4e&quot;));</span>
<span class="fc" id="L239">        System.out.printf(&quot;fractions (eig or s) of total = %s\n&quot;, FormatArray.toString(fracs, &quot;%.4e&quot;));</span>
<span class="fc" id="L240">        System.out.printf(&quot;cumulative proportion=\n%s\n&quot;, FormatArray.toString(c, &quot;%.4e&quot;));</span>

<span class="fc" id="L242">        return calcPrincipalComponents(x, nComponents, u, s, vT, eps);</span>
    }

    /**
     * calculate the principal components of the unit standardized data x
     * using Singular Value Decomposition or CUR Decomposition.
     * NOTE: the data need to be zero centered first.
     *
     &lt;pre&gt;
     the method follows:
     http://www.cs.toronto.edu/~jepson/csc420/
     combined with the book by Strang &quot;Introduction to Linear Algebra&quot;
     and the book by Leskovec, Rajaraman, and Ullman &quot;Mining of Massive Datasets&quot;.
     also useful:
     https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca
     and https://online.stat.psu.edu/stat505/book/export/html/670
     &lt;/pre&gt;
     &lt;pre&gt;
     NOTE:
     variance-covariance(standardized data) == correlation(unstandardized data) == correlation(zero mean centered data).
     therefore, pca using the standardized data == pca using the correlation matrix.
     Also, the eigen of cov(standardized) == eigen of cor(unstandardized).
     &lt;/pre&gt;
     @param x is a 2-dimensional array of k vectors of length n in format
     *    double[n][k].  n is the number of samples, and k is the number of
     *    variables, a.k.a. dimensions.
     *    x should be zero-centered (mean=0) OR standardized to unit normalization (which is mean=0, stdev=1).
     *    If the variance and scale of the variables are different, then unit standard normalization should be used.
     *         Note that using &quot;zero mean centered&quot; x is equivalent to pca on the covariance matrix,
      *      *        while using &quot;unit standard norm: x is equivalent to pca on the correlation matrix.
     @param nComponents the number of principal components to return.
     @param useCUR if true, uses CUR decomposition instead of Singular Value Decomposition.  CUR decomposition
     is useful for very large datasets.
     @return the principal axes, the principal components, and
     a few statistics of the CUR decomposition of A, up to the
     nComponents dimension.  Note that if the rank of the A is
     less than nComponents, then only the number of components as rank is returned.
     * @throws no.uib.cipr.matrix.NotConvergedException
     */
    public static PCAStats calcPrincipalComponents(double[][] x, int nComponents, boolean useCUR) throws NotConvergedException {

<span class="fc" id="L283">        int n = x.length;</span>
<span class="fc" id="L284">        int nDimensions = x[0].length;</span>

<span class="pc bpc" id="L286" title="1 of 2 branches missed.">        if (nComponents &gt; nDimensions) {</span>
<span class="nc" id="L287">            throw new IllegalArgumentException(&quot;x has &quot; + nDimensions + &quot; but &quot;</span>
                    + &quot; nComponents principal components were requested.&quot;);
        }

<span class="fc" id="L291">        double eps = 1.e-15;</span>

        // U is mxm
        // S is mxn
        // V is nxn

        double[][] u;
        double[] s;
        double[][] vT;
<span class="fc bfc" id="L300" title="All 2 branches covered.">        if (!useCUR) {</span>
<span class="fc" id="L301">            SVD svd = SVD.factorize(new DenseMatrix(x));</span>
<span class="fc" id="L302">            s = svd.getS();</span>
<span class="fc" id="L303">            vT = MatrixUtil.convertToRowMajor(svd.getVt());</span>
<span class="fc" id="L304">            u = MatrixUtil.convertToRowMajor(svd.getU());</span>
<span class="fc" id="L305">        } else {</span>
<span class="fc" id="L306">            CURDecomposition.CUR cur = CURDecomposition.calculateDecomposition(x, nComponents);</span>
<span class="fc" id="L307">            MatrixUtil.SVDProducts svd = cur.getApproximateSVD();</span>
<span class="fc" id="L308">            s = svd.s;</span>
<span class="fc" id="L309">            vT = svd.vT;</span>
<span class="fc" id="L310">            u = svd.u;</span>
        }

<span class="fc" id="L313">        return calcPrincipalComponents(x, nComponents, u, s, vT, eps);</span>
    }

    private static PCAStats calcPrincipalComponents(double[][] x, int nComponents,
        double[][] u, double[] s, double[][] vT, double eps) {

<span class="fc" id="L319">        int n = x.length;</span>
<span class="fc" id="L320">        int nDimensions = x[0].length;</span>

<span class="pc bpc" id="L322" title="1 of 2 branches missed.">        if (nComponents &gt; nDimensions) {</span>
<span class="nc" id="L323">            throw new IllegalArgumentException(&quot;x has &quot; + nDimensions + &quot; but &quot;</span>
                    + &quot; nComponents principal components were requested.&quot;);
        }

        // U is mxm
        // S is mxn
        // V is nxn

        int i;
        int j;
<span class="fc" id="L333">        int rank = 0;</span>
<span class="fc bfc" id="L334" title="All 2 branches covered.">        for (i = 0; i &lt; s.length; ++i) {</span>
<span class="pc bpc" id="L335" title="1 of 2 branches missed.">            if (Math.abs(s[i]) &gt; eps) {</span>
<span class="fc" id="L336">                rank++;</span>
            }
        }

<span class="fc bfc" id="L340" title="All 2 branches covered.">        if (nComponents &gt; rank) {</span>
<span class="fc" id="L341">            nComponents = rank;</span>
        }

        // re-doing these for output stats

<span class="fc" id="L346">        double sumEvTrunc = 0;</span>
<span class="fc" id="L347">        double[] eig = new double[rank];</span>
<span class="fc" id="L348">        double sumEVTotal = 0;</span>
<span class="fc" id="L349">        double sumSTotal = 0;</span>
<span class="fc bfc" id="L350" title="All 2 branches covered.">        for (i = 0; i &lt; eig.length; ++i) {</span>
<span class="fc" id="L351">            eig[i] = (s[i] * s[i])/((double)n - 1.);</span>
<span class="fc" id="L352">            sumEVTotal += eig[i];</span>
<span class="fc" id="L353">            sumSTotal += s[i];</span>
<span class="fc bfc" id="L354" title="All 2 branches covered.">            if (i &lt; nComponents) {</span>
<span class="fc" id="L355">                sumEvTrunc = sumEVTotal;</span>
            }
        }

<span class="fc" id="L359">        double[] eigFracs = new double[rank];</span>
<span class="fc" id="L360">        double[] sFracs = new double[rank];</span>
<span class="fc bfc" id="L361" title="All 2 branches covered.">        for (j = 0; j &lt; eig.length; ++j) {</span>
<span class="fc" id="L362">            eigFracs[j] = eig[j]/sumEVTotal;</span>
<span class="fc" id="L363">            sFracs[j] = s[j]/sumSTotal;</span>
        }
        //residual fractional eigenvalue is 1-(sum_{i=0 to nComponents-1}(s[i]) / sum_{i=0 to n-1}(s[i]))
<span class="fc" id="L366">        double residFracEigen = 1 - (sumEvTrunc/sumEVTotal);</span>

<span class="fc" id="L368">        double[] cEig = Arrays.copyOf(eigFracs, eigFracs.length);</span>
<span class="fc bfc" id="L369" title="All 2 branches covered.">        for (j = 1; j &lt; cEig.length; ++j) {</span>
<span class="fc" id="L370">            cEig[j] += cEig[j-1];</span>
        }
<span class="fc" id="L372">        double[] cSingular = Arrays.copyOf(sFracs, sFracs.length);</span>
<span class="fc bfc" id="L373" title="All 2 branches covered.">        for (j = 1; j &lt; cSingular.length; ++j) {</span>
<span class="fc" id="L374">            cSingular[j] += cSingular[j-1];</span>
        }

        // COLUMNS of v are the principal axes, a.k.a. principal directions

<span class="fc" id="L379">        double[][] pA = MatrixUtil.copySubMatrix(vT, 0, nComponents - 1, 0, vT[0].length - 1);</span>

        // X*V = U * diag(s) = principal components
<span class="fc" id="L382">        double[][] uP = MatrixUtil.zeros(u.length, nComponents);</span>
        // principal components for &quot;1 sigma&quot;.  if need 2 sigma, multiply pc by 2,...
<span class="fc" id="L384">        double[][] pC = MatrixUtil.zeros(u.length, nComponents);</span>
<span class="fc bfc" id="L385" title="All 2 branches covered.">        for (i = 0; i &lt; u.length; ++i) {</span>
<span class="fc bfc" id="L386" title="All 2 branches covered.">            for (j = 0; j &lt; nComponents; ++j) {</span>
                // row u_i * diag(s[j])
<span class="fc" id="L388">                pC[i][j] = u[i][j] * s[j];</span>
<span class="fc" id="L389">                uP[i][j] = u[i][j];</span>
            }
        }
        //checked, same as pC = U * diag(s)
        //double[][] xv = MatrixUtil.multiply(x, MatrixUtil.transpose(vT));
        //System.out.printf(&quot;xV=\n%s\n&quot;, FormatArray.toString(xv, &quot;%.4e&quot;));

<span class="fc" id="L396">        PCAStats stats = new PCAStats();</span>
<span class="fc" id="L397">        stats.nComponents = nComponents;</span>
<span class="fc" id="L398">        stats.principalAxes = pA;</span>
<span class="fc" id="L399">        stats.principalComponents = pC;</span>
<span class="fc" id="L400">        stats.eigenValues = eig;</span>
<span class="fc" id="L401">        stats.uP = uP;</span>
<span class="fc" id="L402">        stats._s = Arrays.copyOf(s, s.length);</span>
<span class="fc" id="L403">        stats.cumulativeProportion = cEig;</span>

<span class="fc" id="L405">        double ssdP = 0;</span>
<span class="fc bfc" id="L406" title="All 2 branches covered.">        for (j = nComponents; j &lt; eig.length; ++j) {</span>
<span class="fc" id="L407">            ssdP += eig[j];</span>
        }
<span class="fc" id="L409">        stats.ssdP = ssdP;</span>
<span class="fc" id="L410">        stats.fractionVariance = (sumEVTotal - ssdP)/sumEVTotal;</span>

<span class="fc" id="L412">        System.out.printf(&quot;ssd_p=%.5e\n&quot;, stats.ssdP);</span>
<span class="fc" id="L413">        System.out.printf(&quot;fractionVariance=%.5e\n&quot;, stats.fractionVariance);</span>
<span class="fc" id="L414">        System.out.printf(&quot;principal axes=\n%s\n&quot;, FormatArray.toString(pA, &quot;%.4e&quot;));</span>
<span class="fc" id="L415">        System.out.printf(&quot;principal projections (=u_p*s)=\n%s\n&quot;, FormatArray.toString(pC, &quot;%.4e&quot;));</span>
<span class="fc" id="L416">        System.out.flush();</span>

<span class="fc" id="L418">        return stats;</span>
    }

    /**
     * reconstruct an image from x, given principal directions.
     * from http://www.cs.toronto.edu/~jepson/csc420/
     * combined with the book by Strang &quot;Introduction to Linear Algebra&quot; 
     * and the book by Leskovec, Rajaraman, and Ullman &quot;Mining of Massive Datasets&quot;
     * &lt;pre&gt;
     *     ⃗r(⃗a_0) = m⃗_s + U_p * ⃗a 
     *         where U_p is the principalDirections matrix,
     *         where m_s is the sample x mean,
     *         where a_0 is
     *            a_0 = arg min_{a} ||⃗x − (m⃗_s + U _p * a)||^2
     * &lt;/pre&gt;
     *
     @param m the means of each column of x (used in the zero-correction of x).  the length is k
     *          where k is the number of columns in x before
     *          dimension reduction.
     @param stats the statistics of the principal axes derived from
     * SVD of the covariance of the training data.
     @return the reconstruction of x.  size is [n x k]
     */
    public static double[][] reconstruct(double[] m, PCAStats stats) {

<span class="fc" id="L443">        int p = stats.nComponents;</span>
<span class="fc" id="L444">        int k = stats._s.length;</span>
<span class="fc" id="L445">        int n = stats.principalComponents.length;</span>

<span class="pc bpc" id="L447" title="1 of 2 branches missed.">        if (n != stats.uP.length) {</span>
<span class="nc" id="L448">            throw new IllegalArgumentException(&quot;n must be stats.principalComponents.length which must be equal to &quot; +</span>
                    &quot;stats.principalComponents.length&quot;);
        }
<span class="pc bpc" id="L451" title="1 of 2 branches missed.">        if (k != m.length) {</span>
<span class="nc" id="L452">            throw new IllegalArgumentException(&quot;m.length must equal k == stats.principalAxes.length&quot;);</span>
        }

        //U_p is stats.uP

        //find the 'a' which minimizes ||⃗x − (m⃗_s + p * a)||^2
        //then the reconstruction is r(⃗a ) = m⃗ + U ⃗a

        // U_p is [n X p]
        // x_j is [n X 1]
        // a_j is [p X 1]
        // A is [p X k]

        // reconstruction of x0 is
        //   U_p * a_0
        //   [n X p] * [p X 1] = [n X 1];

        int i;
        int j;

        // pC = SVD(X).U * diag(SVD(X).s) for first p cols of U
        // pA = first k rows of V^T
<span class="fc" id="L474">        double[][] re = MatrixUtil.multiply(stats.principalComponents, stats.principalAxes);</span>
        //reconstructions = stats.principalComponents dot stats.pA + column means
        //    which is u_P * s dot pA
        //[n X p] * [p X k] = [n X k]
<span class="fc bfc" id="L478" title="All 2 branches covered.">        for (i = 0; i &lt; re.length; ++i) {</span>
<span class="fc bfc" id="L479" title="All 2 branches covered.">            for (j = 0; j &lt; re[i].length; ++j) {</span>
<span class="fc" id="L480">                re[i][j] += m[j];</span>
            }
        }
<span class="fc" id="L483">        return re;</span>
    }
    
    /**
     *
     */
    public static class PCAStats {

        /** the number of components requested from the calculation, unless larger than the matrix
         * rank in which case the value will be the rank.
         */
        int nComponents;

        /**
         * principal axes a.k.a. principal directions of the data.
         * These are the first nComponents columns of the SVD(X).V matrix.
         * When considering X as sample covariance, Note that SVD(A^T*A).U and V are both == SVD(A).V.
         * the size is [k X k] where k is x[0].length.
         */
        double[][] principalAxes;

        /**
         * principal components are the data projected onto the principal axes, formed from the left
         eigenvectors dot singular values.
         &lt;pre&gt;
          principal components = first p columns of U times first p elements of diagonal S:
                               = SVD(X).U * diag(SVD(X).s)
         &lt;/pre&gt;
         * The size is [n X p] where n is x.length and p is the number of components.
         */
        double[][] principalComponents;
        
        /**
         * the fraction of the total variance Q_p (where p is the dimension number, that is, nComponents):
         * &lt;pre&gt;
         *    Q_p = (SSD_0 − SSD_p)/SSD_0
         * 
         *    where SSD_p = summation_{j=p+1 to n}(s_j)
         *       where s_j is from the diagonal matrix S of the SVD
         *          of the covariance of x.
         * &lt;/pre&gt;
         */
        double fractionVariance;
        
        /**
         * the cumulative addition of 
         *     (sum_of_eigenvalues - eigenvalue_i)/sum_of_eigenvalues
         *     after truncation.  that is, the sum of eigenvalues is the sum of the truncated number of eigenvalues,
         *      not the entire number from the original decomposition.
         */
        double[] cumulativeProportion;
        
        /**
         * Minimum residual variance
         */
        double ssdP;
        
        /**
         * the first nComponents number of eigenValues
         */
        double[] eigenValues;

        /**
         * the singular values of SVD(A)
         */
        double[] _s;

        /**
         * the first p eigenvectors of the left eigenvector matrix
         * (which is p columns of U from A = SVD(A).U * SVD(A).D * SVD(A).V^T
         */
        double[][] uP;

    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.13.202504020838</span></div></body></html>