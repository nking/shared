<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>PrincipalComponents.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Jacoco Report</a> &gt; <a href="index.source.html" class="el_package">algorithms.dimensionReduction</a> &gt; <span class="el_source">PrincipalComponents.java</span></div><h1>PrincipalComponents.java</h1><pre class="source lang-java linenums">package algorithms.dimensionReduction;

import algorithms.correlation.BruteForce;
import algorithms.matrix.MatrixUtil;
import java.util.Arrays;

import algorithms.statistics.Covariance;
import algorithms.statistics.Standardization;
import algorithms.util.FormatArray;
import no.uib.cipr.matrix.DenseMatrix;
import no.uib.cipr.matrix.NotConvergedException;
import no.uib.cipr.matrix.SVD;

/**
 * find the principal components, that is, the minimal residual variance bases
 * of vectors of samples following G. Strang's SVD in machine learning in
 * http://www.cs.toronto.edu/~jepson/csc420/
 * 
 * from wikipedia:
 * The principal components of a collection of points in a real coordinate space 
 * are a sequence of p unit vectors, where the i-th vector is the direction of 
 * a line that best fits the data while being orthogonal to the first i-1 vectors. 
 * Here, a best-fitting line is defined as one that minimizes the average 
 * squared distance from the points to the line. These directions constitute an 
 * orthonormal basis in which different individual dimensions of the data are 
 * linearly uncorrelated. Principal component analysis (PCA) is the process of 
 * computing the principal components and using them to perform a change of basis 
 * on the data, sometimes using only the first few principal components and 
 * ignoring the rest.
 * 
 * from wikipedia:
 * 
 * PCA is closely related to Fisher's Discriminant Anaylsisi a.k.a. 
 * Linear Discriminant analysis and factor analysis in that they both look for 
 * linear combinations of variables which best explain the data.[4] 
 * LDA explicitly attempts to model the difference between the classes of data. 
 * PCA, in contrast, does not take into account any difference in class, 
 * and factor analysis builds the feature combinations based on differences 
 * rather than similarities.
 * NOTE:  Fisher's original article[1] actually describes a slightly different 
 * discriminant, which does not make some of the assumptions of LDA such as 
 * normally distributed classes or equal class covariances.
 * 
 * LDA is closely related to analysis of variance (ANOVA) and regression analysis, 
 * which also attempt to express one dependent variable as a linear combination 
 * of other features or measurements.[1][2] However, ANOVA uses categorical 
 * independent variables and a continuous dependent variable, whereas 
 * discriminant analysis has continuous independent variables and a categorical 
 * dependent variable (i.e. the class label).[3] Logistic regression and 
 * probit regression are more similar to LDA than ANOVA is, as they also 
 * explain a categorical variable by the values of continuous independent 
 * variables. These other methods are preferable in applications where it is 
 * not reasonable to assume that the independent variables are normally 
 * distributed, which is a fundamental assumption of the LDA method.
 * 
 * @author nichole
 */
<span class="nc" id="L58">public class PrincipalComponents {</span>

    /**
     * calculate the principal components of the unit standardized data x
     * using SVD.
     * NOTE: the data need to be zero centered first.
     *
     &lt;pre&gt;
     the method follows:
     http://www.cs.toronto.edu/~jepson/csc420/
     combined with the book by Strang &quot;Introduction to Linear Algebra&quot;
     and the book by Leskovec, Rajaraman, and Ullman &quot;Mining of Massive Datasets&quot;.
     also useful:
     https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca
     and https://online.stat.psu.edu/stat505/book/export/html/670
     &lt;/pre&gt;
     &lt;pre&gt;
     NOTE:
     variance-covariance(standardized data) == correlation(unstandardized data) == correlation(zero mean centered data).
     therefore, pca using the standardized data == pca using the correlation matrix.
     Also, the eigen of cov(standardized) == eigen of cor(unstandardized).
     &lt;/pre&gt;
     * @param x is a 2-dimensional array of k vectors of length n in format
     *    double[n][k].  n is the number of samples, and k is the number of
     *    variables, a.k.a. dimensions.
     *    x should be zero-centered (mean=0) OR standardized to unit normalization (which is mean=0, stdev=1).
     *    If the variance and scale of the variables are different, then unit standard normalization should be used.
     @param nComponents the number of principal components to return.
     @return the principal axes, the principal components, and
     a few statistics of the CUR decomposition of A, up to the
     nComponents dimension.  Note that if the rank of the A is
     less than nComponents, then only the number of components as rank is returned.
     */
    public static PCAStats calcPrincipalComponents(double[][] x, int nComponents) throws NotConvergedException {

<span class="fc" id="L93">        int n = x.length;</span>
<span class="fc" id="L94">        int nDimensions = x[0].length;</span>

<span class="pc bpc" id="L96" title="1 of 2 branches missed.">        if (nComponents &gt; nDimensions) {</span>
<span class="nc" id="L97">            throw new IllegalArgumentException(&quot;x has &quot; + nDimensions + &quot; but &quot;</span>
                    + &quot; nComponents principal components were requested.&quot;);
        }

<span class="fc" id="L101">        double eps = 1.e-15;</span>

        /*
        minimal residual variance basis:
           basis direction b.
           samples are vectors x_j where j = 0 to k
           sample mean m_s = (1/k) summation_over_j(x_j)

           looking for a p-dimensional basis that minimizes:
              SSD_p = min_{B}( summation_over_j( min_{a_j}(||x_j - (m_s + B*a_j)||^2) ) )
                 where B = (b1, . . . , bp) is the n × p matrix formed from the selected basis.

              choose the coefficients ⃗aj which minimize the least squares error
                 E_j^2 = ||x_j - (m_s + B*a_j)||^2
              then choose B to minimize the SSD = summation_over_j(E_j^2)

           SSD_p is called the minimum residual variance for any basis of dimension p.

           SVD( Sample Covariance ) = SVD((1/(n-1)) * X^T*X)

           SVD(A^T*A).U and V are both == SVD(A).V
           SVD(A*A^T).U and V are both == SVD(A).U

             A = U * D * V^T  where all U and V^T are from SVD(A), and D is from SVD(A).s
             A^T A = V * D^2 * V^T
             SampleCov = (1/(n-1)) * A^T A
                       = (1/(n-1)) * V * D^2 * V^T

             we also have U * D = X * V from SVD

             Jepson:  B first vector is the first column of V in SVD(C_S)
                where C_S is the sample covariance and SVD(C_S).V == SVD(C_S).U ~ SVD(X).V

         SVD(C_S) = SVD((1/(n-1)) * X^T*X)
         SVD(X^T*X).U == SVD(X).V
            V gives the principal axes or principal directions of the dat
            X*V = U * D = principal components = projection of the data onto the principal axes
               and the coords of the newly transformed data are on row_0 for the first data point, etc.
        */

<span class="fc" id="L141">        SVD svd = SVD.factorize(new DenseMatrix(x));</span>

        // U is mxm
        // S is mxn
        // V is nxn

<span class="fc" id="L147">        double[] s = svd.getS();</span>
        int i;
        int j;
<span class="fc" id="L150">        int rank = 0;</span>
<span class="fc bfc" id="L151" title="All 2 branches covered.">        for (i = 0; i &lt; s.length; ++i) {</span>
<span class="pc bpc" id="L152" title="1 of 2 branches missed.">            if (Math.abs(s[i]) &gt; eps) {</span>
<span class="fc" id="L153">                rank++;</span>
            }
        }

<span class="pc bpc" id="L157" title="1 of 2 branches missed.">        if (nComponents &gt; rank) {</span>
<span class="nc" id="L158">            nComponents = rank;</span>
        }

<span class="fc" id="L161">        double sumEvTrunc = 0;</span>
<span class="fc" id="L162">        double[] eig = new double[rank];</span>
<span class="fc" id="L163">        double sumEVTotal = 0;</span>
<span class="fc bfc" id="L164" title="All 2 branches covered.">        for (i = 0; i &lt; eig.length; ++i) {</span>
<span class="fc" id="L165">            eig[i] = (s[i] * s[i])/((double)n - 1.);</span>
<span class="fc" id="L166">            sumEVTotal += eig[i];</span>
<span class="pc bpc" id="L167" title="1 of 2 branches missed.">            if (i &lt; rank) {</span>
<span class="fc" id="L168">                sumEvTrunc = sumEVTotal;</span>
            }
        }
        //residual fractional eigenvalue is 1-(sum_{i=0 to nComponents-1}(s[i]) / sum_{i=0 to n-1}(s[i]))
<span class="fc" id="L172">        double residFracEigen = 1 - (sumEvTrunc/sumEVTotal);</span>

        // COLUMNS of v are the principal axes, a.k.a. principal directions
<span class="fc" id="L175">        double[][] vT = MatrixUtil.convertToRowMajor(svd.getVt());</span>
<span class="fc" id="L176">        double[][] pA = MatrixUtil.copySubMatrix(vT, 0, nComponents - 1, 0, vT[0].length - 1);</span>

        // // X*V = U * diag(s) = principal components
<span class="fc" id="L179">        DenseMatrix u = svd.getU();</span>
        // principal components for &quot;1 sigma&quot;.  if need 2 sigma, multiply pc by 2,...
<span class="fc" id="L181">        double[][] pC = MatrixUtil.zeros(u.numRows(), nComponents);</span>
<span class="fc bfc" id="L182" title="All 2 branches covered.">        for (i = 0; i &lt; u.numRows(); ++i) {</span>
<span class="fc bfc" id="L183" title="All 2 branches covered.">            for (j = 0; j &lt; nComponents; ++j) {</span>
                // row u_i * diag(s[j])
<span class="fc" id="L185">                pC[i][j] = u.get(i, j) * svd.getS()[j];</span>
            }
        }
        //checked, same as pC = U * diag(s)
        //double[][] xv = MatrixUtil.multiply(x, MatrixUtil.transpose(vT));
        //System.out.printf(&quot;xV=\n%s\n&quot;, FormatArray.toString(xv, &quot;%.4e&quot;));

<span class="fc" id="L192">        PCAStats stats = new PCAStats();</span>
<span class="fc" id="L193">        stats.nComponents = nComponents;</span>
<span class="fc" id="L194">        stats.principalAxes = pA;</span>
<span class="fc" id="L195">        stats.principalComponents = pC;</span>
<span class="fc" id="L196">        stats.eigenValues = eig;</span>

<span class="fc" id="L198">        double total = 0;</span>
<span class="fc bfc" id="L199" title="All 2 branches covered.">        for (j = 0; j &lt; eig.length; ++j) {</span>
<span class="fc" id="L200">            total += eig[j];</span>
        }
<span class="fc" id="L202">        double[] fracs = new double[rank];</span>
<span class="fc bfc" id="L203" title="All 2 branches covered.">        for (j = 0; j &lt; rank; ++j) {</span>
<span class="fc" id="L204">            fracs[j] = eig[j]/total;</span>
        }
<span class="fc" id="L206">        double[] c = Arrays.copyOf(fracs, fracs.length);</span>
<span class="fc bfc" id="L207" title="All 2 branches covered.">        for (j = 1; j &lt; c.length; ++j) {</span>
<span class="fc" id="L208">            c[j] += c[j-1];</span>
        }
<span class="fc" id="L210">        stats.cumulativeProportion = c;</span>

<span class="fc" id="L212">        double ssdP = 0;</span>
<span class="fc bfc" id="L213" title="All 2 branches covered.">        for (j = nComponents; j &lt; nDimensions; ++j) {</span>
<span class="fc" id="L214">            ssdP += eig[j];</span>
        }
<span class="fc" id="L216">        stats.ssdP = ssdP;</span>
<span class="fc" id="L217">        stats.fractionVariance = (total - ssdP)/total;</span>

<span class="fc" id="L219">        System.out.printf(&quot;ssd_p=%.5e\n&quot;, stats.ssdP);</span>
<span class="fc" id="L220">        System.out.printf(&quot;fractionVariance=%.5e\n&quot;, stats.fractionVariance);</span>

<span class="fc" id="L222">        System.out.printf(&quot;eig = %s\n&quot;, FormatArray.toString(eig, &quot;%.4e&quot;));</span>
<span class="fc" id="L223">        System.out.printf(&quot;s fractions of total = \n%s\n&quot;,</span>
<span class="fc" id="L224">                FormatArray.toString(fracs, &quot;%.4e&quot;));</span>
<span class="fc" id="L225">        System.out.printf(&quot;eigenvalue cumulativeProportion=\n%s\n&quot;,</span>
<span class="fc" id="L226">                FormatArray.toString(stats.cumulativeProportion, &quot;%.4e&quot;));</span>
<span class="fc" id="L227">        System.out.printf(&quot;principal axes=\n%s\n&quot;, FormatArray.toString(pA, &quot;%.4e&quot;));</span>
<span class="fc" id="L228">        System.out.printf(&quot;principal projections (=u_p*s)=\n%s\n&quot;, FormatArray.toString(pC, &quot;%.4e&quot;));</span>
<span class="fc" id="L229">        System.out.flush();</span>

        //checked, same as pC = U * diag(s)
        //double[][] xv = MatrixUtil.multiply(x, MatrixUtil.transpose(vT));
        //System.out.printf(&quot;xV=\n%s\n&quot;, FormatArray.toString(xv, &quot;%.4e&quot;));

<span class="fc" id="L235">        return stats;</span>
    }

    /**
     * calculate the principal components of the unit standardized data x
     * using CUR decomposition.
     * NOTE: the data need to be zero centered first.
     *
     &lt;pre&gt;
     the method follows:
     http://www.cs.toronto.edu/~jepson/csc420/
     combined with the book by Strang &quot;Introduction to Linear Algebra&quot;
     and the book by Leskovec, Rajaraman, and Ullman &quot;Mining of Massive Datasets&quot;.
     http://www.mmds.org/mmds/v2.1/ch11-dimred.pdf
     also useful:
     https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca
     and https://online.stat.psu.edu/stat505/book/export/html/670
     &lt;/pre&gt;
     &lt;pre&gt;
     NOTE:
     variance-covariance(standardized data) == correlation(unstandardized data) == correlation(zero mean centered data).
     therefore, pca using the standardized data == pca using the correlation matrix.
     Also, the eigen of cov(standardized) == eigen of cor(unstandardized).
     &lt;/pre&gt;
     * @param x is a 2-dimensional array of k vectors of length n in format
     *    double[n][k].  n is the number of samples, and k is the number of
     *    variables, a.k.a. dimensions.
     *          x should be zero-centered (mean=0) OR standardized to unit normalization (which is mean=0, stdev=1).
     *      If the variance and scale of the variables are different, then unit standard normalization should be used.
     * @param nComponents the number of principal components to return.
     * @return the principal axes, the principal components, and
     * a few statistics of the CUR decomposition of A, up to the
     * nComponents dimension.  Note that if the rank of the A is
     * less than nComponents, then only the number of components as rank is returned.
     */
    public static PCAStats calcPrincipalComponentsUsingCURDecomp(double[][] x, int nComponents) throws NotConvergedException {

<span class="nc" id="L272">        int n = x.length;</span>
<span class="nc" id="L273">        int nDimensions = x[0].length;</span>
        
<span class="nc bnc" id="L275" title="All 2 branches missed.">        if (nComponents &gt; nDimensions) {</span>
<span class="nc" id="L276">            throw new IllegalArgumentException(&quot;x has &quot; + nDimensions + &quot; but &quot; </span>
                + &quot; nComponents principal components were requested.&quot;);
        }
        
<span class="nc" id="L280">        double eps = 1.e-15;</span>

<span class="nc" id="L282">        CURDecomposition.CUR cur = CURDecomposition.calculateDecomposition(x, nComponents);</span>

        // U is mxm
        // S is mxn
        // V is nxn

<span class="nc" id="L288">        MatrixUtil.SVDProducts svd = cur.getApproximateSVD();</span>
<span class="nc" id="L289">        double[] s = svd.s;</span>
        int i;
        int j;
<span class="nc" id="L292">        int rank = 0;</span>
<span class="nc bnc" id="L293" title="All 2 branches missed.">        for (i = 0; i &lt; s.length; ++i) {</span>
<span class="nc bnc" id="L294" title="All 2 branches missed.">            if (Math.abs(s[i]) &gt; eps) {</span>
<span class="nc" id="L295">                rank++;</span>
            }
        }
        
<span class="nc bnc" id="L299" title="All 2 branches missed.">        if (nComponents &gt; rank) {</span>
<span class="nc" id="L300">            nComponents = rank;</span>
        }

<span class="nc" id="L303">        double sumEvTrunc = 0;</span>
<span class="nc" id="L304">        double[] eig = new double[rank];</span>
<span class="nc" id="L305">        double sumEVTotal = 0;</span>
<span class="nc bnc" id="L306" title="All 2 branches missed.">        for (i = 0; i &lt; eig.length; ++i) {</span>
<span class="nc" id="L307">            eig[i] = (s[i] * s[i])/((double)n - 1.);</span>
<span class="nc" id="L308">            sumEVTotal += eig[i];</span>
<span class="nc bnc" id="L309" title="All 2 branches missed.">            if (i &lt; rank) {</span>
<span class="nc" id="L310">                sumEvTrunc = sumEVTotal;</span>
            }
        }
        //residual fractional eigenvalue is 1-(sum_{i=0 to nComponents-1}(s[i]) / sum_{i=0 to n-1}(s[i]))
<span class="nc" id="L314">        double residFracEigen = 1 - (sumEvTrunc/sumEVTotal);</span>

        // COLUMNS of v are the principal axes, a.k.a. principal directions
<span class="nc" id="L317">        double[][] vT = svd.vT;</span>
<span class="nc" id="L318">        double[][] pA = MatrixUtil.copySubMatrix(vT, 0, nComponents - 1, 0, vT[0].length - 1);</span>

        // // X*V = U * diag(s) = principal components
        // principal components for &quot;1 sigma&quot;.  if need 2 sigma, multiply pc by 2,...
<span class="nc" id="L322">        double[][] pC = MatrixUtil.zeros(svd.u.length, nComponents);</span>
<span class="nc bnc" id="L323" title="All 2 branches missed.">        for (i = 0; i &lt; svd.u.length; ++i) {</span>
<span class="nc bnc" id="L324" title="All 2 branches missed.">            for (j = 0; j &lt; nComponents; ++j) {</span>
                // row u_i * diag(s[j])
<span class="nc" id="L326">                pC[i][j] = svd.u[i][j] * svd.s[j];</span>
            }
        }

<span class="nc" id="L330">        PCAStats stats = new PCAStats();</span>
<span class="nc" id="L331">        stats.nComponents = nComponents;</span>
<span class="nc" id="L332">        stats.principalAxes = pA;</span>
<span class="nc" id="L333">        stats.principalComponents = pC;</span>
<span class="nc" id="L334">        stats.eigenValues = eig;</span>

<span class="nc" id="L336">        double total = 0;</span>
<span class="nc bnc" id="L337" title="All 2 branches missed.">        for (j = 0; j &lt; eig.length; ++j) {</span>
<span class="nc" id="L338">            total += eig[j];</span>
        }
<span class="nc" id="L340">        double[] fracs = new double[rank];</span>
<span class="nc bnc" id="L341" title="All 2 branches missed.">        for (j = 0; j &lt; rank; ++j) {</span>
<span class="nc" id="L342">            fracs[j] = eig[j]/total;</span>
        }
<span class="nc" id="L344">        double[] c = Arrays.copyOf(fracs, fracs.length);</span>
<span class="nc bnc" id="L345" title="All 2 branches missed.">        for (j = 1; j &lt; c.length; ++j) {</span>
<span class="nc" id="L346">            c[j] += c[j-1];</span>
        }
<span class="nc" id="L348">        stats.cumulativeProportion = c;</span>
        
<span class="nc" id="L350">        double ssdP = 0;</span>
<span class="nc bnc" id="L351" title="All 2 branches missed.">        for (j = nComponents; j &lt; nDimensions; ++j) {</span>
<span class="nc" id="L352">            ssdP += eig[j];</span>
        }
<span class="nc" id="L354">        stats.ssdP = ssdP;</span>
<span class="nc" id="L355">        stats.fractionVariance = (total - ssdP)/total;</span>

<span class="nc" id="L357">        System.out.printf(&quot;ssd_p=%.5e\n&quot;, stats.ssdP);</span>
<span class="nc" id="L358">        System.out.printf(&quot;fractionVariance=%.5e\n&quot;, stats.fractionVariance);</span>

<span class="nc" id="L360">        System.out.printf(&quot;eig=%s\n&quot;, FormatArray.toString(eig, &quot;%.4e&quot;));</span>
<span class="nc" id="L361">        System.out.printf(&quot;s fractions of total = \n%s\n&quot;,</span>
<span class="nc" id="L362">                FormatArray.toString(fracs, &quot;%.4e&quot;));</span>
<span class="nc" id="L363">        System.out.printf(&quot;eigenvalue cumulativeProportion=\n%s\n&quot;,</span>
<span class="nc" id="L364">                FormatArray.toString(stats.cumulativeProportion, &quot;%.4e&quot;));</span>
<span class="nc" id="L365">        System.out.printf(&quot;principal axes=\n%s\n&quot;, FormatArray.toString(pA, &quot;%.4e&quot;));</span>
<span class="nc" id="L366">        System.out.printf(&quot;principal components (=u_p*s)=\n%s\n&quot;, FormatArray.toString(pC, &quot;%.4e&quot;));</span>
<span class="nc" id="L367">        System.out.flush();</span>

<span class="nc" id="L369">        return stats;</span>
    }
    
    /**
     * reconstruct an image from x, given principal directions.
     * from http://www.cs.toronto.edu/~jepson/csc420/
     * combined with the book by Strang &quot;Introduction to Linear Algebra&quot; 
     * and the book by Leskovec, Rajaraman, and Ullman &quot;Mining of Massive Datasets&quot;
     * &lt;pre&gt;
     *     ⃗r(⃗a_0) = m⃗_s + U_p * ⃗a 
     *         where U_p is the principalDirections matrix,
     *         where m_s is the sample x mean,
     *         where a_0 is
     *            a_0 = arg min_{a} ||⃗x − (m⃗_s + U _p * a)||^2
     * &lt;/pre&gt;
     * @param x a 2-dimensional array of k vectors of length n in format
     *    double[n][k] which is double[nSamples][nDimensions] for which to apply
     *    the principal axes transformation on.
     * @param stats the statistics of the principal axes derived from
     * SVD of the covariance of the training data.
     * @return 
     */
    public static double[][] reconstruct(double[][] x, PCAStats stats) {
        
        /* find the 'a' which minimizes ||⃗x − (m⃗_s + p * a)||^2
        ⃗
        then the reconstruction is r(⃗a ) = m⃗ + U ⃗a 
        */

        //TODO: revisit this

<span class="nc" id="L400">        double[][] b = stats.principalComponents;</span>
        
<span class="nc" id="L402">        return b;</span>
    }
    
    public static class PCAStats {

        /** the number of components requested from the calculation, unless larger than the matrix
         * rank in which case the value will be the rank.
         */
        int nComponents;

        /**
         * principal axes a.k.a. principal directions of the data.
         * These are the first nComponents columns of the SVD V matrix.
         * When considering X as sample covariance, Note that SVD(A^T*A).U and V are both == SVD(A).V.
         */
        double[][] principalAxes;
        
        /**
         * principal components are the data projected onto the principal axes.
         * principal components = X*V = U * D where X = SVD(X).U * diag(SVD(X).s) * SVD(X).vT
         */
        double[][] principalComponents;
        
        /**
         * the fraction of the total variance Q_p (where p is the dimension number, that is, nComponents):
         * &lt;pre&gt;
         *    Q_p = (SSD_0 − SSD_p)/SSD_0
         * 
         *    where SSD_p = summation_{j=p+1 to n}(s_j)
         *       where s_j is from the diagonal matrix S of the SVD
         *          of the covariance of x.
         * &lt;/pre&gt;
         */
        double fractionVariance;
        
        /**
         * the cumulative addition of 
         *     (sum_of_eigenvalues - eigenvalue_i)/sum_of_eigenvalues
         *     after truncation.  that is, the sum of eigenvalues is the sum of the truncated number of eigenvalues,
         *      not the entire number from the original decomposition.
         */
        double[] cumulativeProportion;
        
        /**
         * Minimum residual variance
         */
        double ssdP;
        
        /**
         * the first nComponents number of eigenValues
         */
        double[] eigenValues;
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.8.202204050719</span></div></body></html>